% is is a part of the habilitation thesis of Max Neunhoeffer

\chapter{Matrices over finite fields}

This chapter covers the implementation of the basic operations for matrices 
over finite fields. We begin with a description of a new concrete 
representation of such matrices on nowadays computers in 
Section~\ref{sec:ffematrices}, both in main memory and on storage.
We then analyse the performance and complexity of matrix arithmetic 
for this new representation in Section~\ref{sec:matarith} and compare
it to previous implementations. 
%In Section~\ref{sec:basalgmat}, we give
%an overview over the most basic algorithms for finite field matrices.

We conclude our presentation of fundamental matrix algorithms
in the next chapter where we present a method to compute the characteristic
polynomial of matrices over finite fields and a new method
for the minimal polynomial, which is used to compute projective orders
of matrices as will be shown in Chapter~\ref{chap:linalg}.

%FIXME

\section{Representing vectors and matrices over finite fields}
\label{sec:ffematrices}

If you already know something about compact vectors and matrices over 
finite fields you can safely skip the next subsection and directly
proceed to Section~\ref{ssec:cvec}. We first explain the basic idea.

\subsection{The idea}

If $p$ is a prime then elements of the finite field $\F_p$ with $p$
elements can be represented by the integers $0, 1, \ldots, p-1$. Thus,
storing one such element on a computer needs only $\lceil \log_2(p)
\rceil$ bits. The finite extension field  $\F_q$ with $q = p^k$ elements 
is built
as quotient $\F_p[x]/c_k \F_p[x]$ using the Conway polynomial
$c_k$ (see \cite{Nickel} or on the web at \cite{ConwayFL}). Since the
Conway polynomials are monic by definition, an element
of\, $\F_q$ can be represented by a polynomial over\, $\F_p$ of degree 
less then $k$ and thus by storing $k$ elements of\, $\F_p$ using $\lceil
\log_2(p^k) \rceil$ bits.

Since linear algebra operations over finite fields can be performed
rather efficiently by modern microprocessors, the limiting factor 
for such computations is memory access. Therefore, it is performance
critical to store vectors and matrices using as little memory as possible
and to choose the data structure in a way that allows for fast memory
access. We call such memory efficient data structures ``compact''.

This idea is quite old (see for example \cite{MeatAxeRP}) and has already 
been used extensively (see for example \cite{CMeatAxe} or \cite{GAP4} or
various other systems).

There is a fundamental difference between the characteristic $p=2$ case
and all other characteristics. The reason for this is that in
characteristic $2$ the addition of vectors can be implemented by using
the XOR (exclusive or) operation available in every microprocessor
instruction set. In odd characteristic, the available instructions
do not fit so well to finite field arithmetic. Therefore, previous
implementations mostly rely on table lookups to perform arithmetic
operations. Since the memory space available for lookup tables is
limited, this method is limited to relatively small fields (usually up to
fields with $256$ elements) and has to use single byte accesses
as opposed to processor word accesses, for which modern machines
seem to be optimised. These limitations seem to become more serious
as word lengths in standard microprocessors increase.

The main novelty in the approach presented here is to overcome this
problem by choosing the data structures in a way that allows to use
processor word operations for all arithmetic operations in all
characteristics. Also this idea has been used before (see
\cite{EssenLinAlg}), however, no implementation of this seems to
be available any more and, more importantly, the approach still
insisted on storing whole elements of\, $\F_q$ in one machine word.
We will pack as many prime field elements as possible into a machine
word and distribute the various prime field coefficients of a single
element of\, $\F_q$ into distinct machine words to allow for better
memory usage in the case of extension fields.


\subsection{Compact vectors}
\label{ssec:cvec}

We first describe in detail how a vector of length $n$ over the field
$\F_q$ with $q=p^k$ elements is stored on a machine with word length $32$ 
or $64$ bits respectively. The ideas for the step from $32$ to $64$ bits
can be applied analogously for future microprocessors with even larger
word length. We ignore architectures with funny word lengths not being
a multiple of $32$ bits.

Let $e$ be $\lceil \log_2(2p-1)\rceil$ for $p > 2$ and $1$ for $p=2$. 
This is the number of bits
necessary to store an integer in the range $0,1, \ldots, 2p-2$ except
for $p=2$. This number $e$ is the number of bits we 
reserve for every prime field element we want to store. For $p=2$ this
is evidently best possible, whereas for odd $p$ we seem to waste
one bit per prime field element, since we seem to need only to store
numbers in the range $0,1,\ldots p-1$. This additional bit in the
data structure allows us to represent a sum of two numbers in the
range $0,1,\ldots, p-1$ using $e$ bits in odd characteristic.

We start with the prime field case $q=p$.

We pack as many prime field elements as possible into a machine
word, that is, $w := \lfloor 32/e \rfloor$ prime field elements per word
on a machine with word length $32$ bit. On machines with word length $64$ bits
we pack $w := 2 \cdot \lfloor 32/e \rfloor$ prime field elements into one word.
Note that we do not use $\lfloor 64/e \rfloor$ elements which can be one
more, since then the conversion between the different data formats for
different word lengths becomes too expensive and awkward.

We always imagine the least significant bit in a machine word as being 
on the right hand side. To store a vector, we start filling machine words 
from right to left, always using $e$ bits for one prime field element and
packing $w$ prime field elements into a word.

We illustrate this layout in the following example for $p=5$ and thus
$e=4$ on a machine with $32$ bit word length:

\begin{verbatim}
  bit             3322|2222|2222|1111|1111|1100|0000|0000
  number:         1098|7654|3210|9876|5432|1098|7654|3210
  prime field         |    |    |    |    |    |    |
  element number: 7777|6666|5555|4444|3333|2222|1111|0000
\end{verbatim}

Within each block of $e$ bits we represent prime field elements
by the binary representation of a number in the range $0,1,\ldots,p-1$
with the least significant bit of this binary representation on the
right hand side. Thus, in our example $1+1+1+1 \in \F_5$ would be
represented by the bit sequence \texttt{0100} and $1+1+1$ by \texttt{0011}.
Note that the most significant bit in this representation is always $0$.

The first $w$ prime field elements in a vector are stored in the first
machine word of the vector, the next $w$ in the next word and so on.

We proceed now to the extension field case $q=p^k$. Let $e$ and $w$ be
exactly the same values as above. We now have to store $k$ prime field
elements for every element of $\F_q = \F_p[x]/c_k\F_p[x]$, namely the 
coefficients of the unique residue class representative of degree smaller 
than $k$, where $c_k$ is the Conway polynomial used to construct the 
finite field extension $\F_q$ over $\F_p$ (for details see \cite{Nickel}
or on the web \cite{ConwayFL}). Namely, if $a \in \F_q$ is represented by
the polynomial $\sum_{i=0}^{k-1} a_i x^i$, then we have to store the prime 
field elements $a_0, a_1, \ldots, a_{k-1}$. 

In a compact vector of length $n$ of elements of $\F_q$ we distribute
those numbers in the following way: The first $k$ machine words in the
memory representation of the vector are used to store the first $w$
elements of the vector. The first machine word holds all the coefficients
$a_0$ of those $\F_q$ elements, the second the coefficients $a_1$ and so
on. Since one machine word can hold up to $w$ prime field elements this
fills the first $k$ words rather satisfactorily. The second $k$ machine words
in the vector then hold the vector elements with indices 
$w+1, w+2, \ldots, 2w$ in exactly the same way.

Note again that for machines with a word length of $64$ bit we choose the
value $w$ only twice as big as for $32$ bit machines, even if one more
prime field element would fit into the machine word.

The only natural limit of this implementation is that 
$\lceil \log_2(2p-1) \rceil$ must be smaller or equal to the word length
of the microprocessor.

We can now explain how we can add vectors in the above representation
only by doing a series of word operations.

\subsection{Adding compact vectors}

The basic addition formula for finite field elements is rather simple:
For prime field elements we only have to add two numbers in the range 
from $0$ to $p-1$, and
subtract $p$, if the sum is greater or equal to $p$. The extension
field elements are done componentwise. However, we have to solve
the problem of doing this simple operation using word operations and 
thus doing this for $w$ prime field elements at the same time.
This is easy for $p=2$, since we can use the standard XOR (exclusive or)
operation.

The idea to overcome this problem for $p > 2$ is the following. 
Assume $a$ and $b$
are two integers in the range from $0$ to $p-1$. By adding
$2^{e-1}-p$ to the sum $a+b$ we get $t := a+b+2^{e-1}-p$, which
has the property, that $t \ge 2^{e-1}$ if and only if $a+b \ge p$
(remember $2p-1 \le 2^e$ and thus $p-1 < 2^{e-1}$). That is, if
the number $t$ is represented using binary expansion with $e$ bits,
then the most significant bit is set if and only if $a+b \ge p$.

This idea is now used for two words $a$ and $b$, containing $w$
prime field elements each. Every prime field element uses exactly $e$ bits
in its word and we call these sections of $e$ adjacent bits in a word 
``components'' for the moment. 

We prepare an ``offset'' word $o$ that contains in each component the
number $2^{e-1}-p$ and a ``mask'' word $m$ that contains in each component
the number $2^{e-1}$ meaning, that in each component only the most significant
bit is set and all others are zero. In addition, we keep a word $n$
containing the number $p$ in each component.

The finite field sum $c$ of $a$ and $b$ is now computed in the following way:
First $s := a+b$ and $t := a+b+o$ are computed. We then use an AND 
operation for words to extract exactly the most significant bits of
those components, in which the sum was greater or equal to $p$.
This is done by computing $r := t \ \&\  m$ (we use the \& symbol to
indicate bitwise AND operations). Bit-shifting
the word $r$ by $e-1$ bits to the right (we use the notation
$r \gg (e-1)$ for this) and subtracting the
result from $r$ now results in a word $u := r - (r \gg (e-1))$
having the number $2^{e-1}-1$
in those components, in which the sum was greater or equal to $p$ and
$0$ in the others. Finally, doing a bitwise AND operation of $u$ with
$n$ results in exactly the right word to subtract from $s$ to get
the correct result.

Thus, the complete formula is
\[ a+b - \Big(\big(r - (r \gg (e-1))\big) \ \&\ n \Big)
   \qquad \mbox{where}\quad r = (a+b+o) \ \&\  m \]

We illustrate this by an example for $p=3$ on a machine with
$32$ bit word length. In this case, $e = 3$ and $w = 10$. We want to
add the words shown below in the rows depicted by \texttt{a} and \texttt{b}.
We also show the prepared words $o$, $m$, and $n$. The bits marked
with \texttt{X} are not used and are all equal to zero.

\begin{verbatim}
  bit             33|222|222|222|211|111|111|110|000|000|000
  number:         10|987|654|321|098|765|432|109|876|543|210
                    |   |   |   |   |   |   |   |   |   |
  a:              XX|000|010|001|000|010|001|000|010|001|000
  b:              XX|000|010|010|010|001|001|001|000|000|000
  o:              XX|001|001|001|001|001|001|001|001|001|001
  m:              XX|100|100|100|100|100|100|100|100|100|100
  n:              XX|011|011|011|011|011|011|011|011|011|011
\end{verbatim}

In the following table we show some intermediate results and repeat
the input values for easier verification:

\begin{verbatim}
  a+b:            XX|000|100|011|010|011|010|001|010|001|000
  a+b+o:          XX|000|101|100|011|100|011|010|011|010|001
  r:              XX|000|100|100|000|100|000|000|000|000|000
  u:              XX|000|011|011|000|011|000|000|000|000|000
  u&n:            XX|000|011|011|000|011|000|000|000|000|000
  result:         XX|000|001|000|010|000|010|001|010|001|000
  a:              XX|000|010|001|000|010|001|000|010|001|000
  b:              XX|000|010|010|010|001|001|001|000|000|000
\end{verbatim}

Of course, it is a coincidence here that $u$ is equal to 
$u \ \&\ n$, since $p = 2^{e-1}-1$.

This means, that the addition of two words containing $w$ prime field
elements can be done in $7$ word operations for $p > 2$. For the
$p=2$ case, we only need one XOR operation. Thus we have proved:

\begin{Prop}[Addition of compact vectors]
\label{addvec}
We assume a machine with $32$ bit word length. For $2 < p < 2^{31}$,
two compact vectors of length $n$ over the field of $q=p^k$
elements can be added using $7k\cdot \lceil n/w \rceil$ word operations
(plus memory fetches and stores), where $w = \lfloor 32/e \rfloor$
and $e = \lceil \log_2(2p-1) \rceil$. 

For $p=2$, only $k \cdot \lceil n/32 \rceil$ word operations are needed.

For machines with $64$ bit word length the number of word operations
is $7k \cdot \lceil n/(2w) \rceil$ and $\lceil n/64 \rceil$ resp.~and 
we can work with primes $p < 2^{63}$.
\end{Prop}
\Proof See above. \ProofEnd

\begin{Rem}
Note that since the amount of memory needed for a vector of length $n$
is $k \cdot \lceil n/w \rceil$ for $p > 2$ and $\lceil n/32 \rceil$ for
$p=2$ this means that the addition needs $7$ word operations for each
word of a vector for $p>2$ and $1$ word operation for $p=2$.

Assuming a microprocessor
with a long enough instruction pipeline we can conclude that all this
can be done as fast as accessing the main memory to fetch $a$ and $b$ and
store the result somewhere, such that the number $7$ does not hurt at all. 
Compare Section~\ref{ssec:discussion} and see Section~\ref{sec:cache}
for some additional comments on processor caches.
\end{Rem}

Next we consider multiplication of vectors by scalars.

\subsection{Multiplication by scalars}

To explain the method, we restrict our attention to the case that one
compact vector shall be multiplied by one scalar and the result 
shall be stored in some other memory location.

Let $e$ and $w$ be defined as in Section~\ref{ssec:cvec}.

We start by discussing the prime field case $\F_p$.
Since a scalar $s \in \F_p$ is represented by an integer in the range $0$ 
to $p-1$ in its binary expansion, we can multiply a compact vector $v$
by $s$ by repeatedly adding vectors to themselves starting with $v$ and
adding up those multiples
whose corresponding bits in the binary expansion of $s$ are set. That is,
if $s = \sum_{i=0}^{e-2} s_i 2^i$ with $s_i \in \{0,1\}$ and again
$e = \lceil \log_2(2p-1) \rceil$, we compute $s\cdot v$ by computing
$v, 2^1 \cdot v, 2^2 \cdot v, \ldots, 2^{e-2} \cdot v$ and then summing
$\sum_{i=0}^{e-2} s_i \cdot (2^i \cdot v)$.
All this can be done with at most $2(e-2)$ vector additions.

We now proceed to the extension field case $\F_q = \F_p[x]/c_k \F_p[x]$ 
with the Conway polynomial $c_k$ and $q = p^k$.
Here again a scalar $s \in \F_q$ is represented by an expansion
$s = \sum_{i=0}^{k-1} s_i x^i + c_k \cdot \F_p[x]$. For the rest of this
section we omit the ``$+ c_k \cdot \F_p[x]$'' and denote cosets
by their representing polynomials of degree less than $k$.

We reduce the problem to scalar multiplications of vectors with
prime field elements. To this end, we have to be able to multiply
a vector with the primitive root $x$.

Considering a single scalar $t = \sum_{i=0}^{k-1} t_i x^i \in \F_q$, 
we see that 
\[ xt = \sum_{i=1}^{k-1} (t_{i-1}x^i) 
+ \sum_{i=0}^{k-1} t_{k-1} \cdot (x^k - c_k) \in \F_q \] 
(remember that we compute in $\F_p[x]$ modulo $c_k$).
Thus, the multiplication by $x$ can be achieved by a shift, one
multiplication of $x^k - c_k$ with the prime field element $t_{k-1}$,
and an addition.

Since we distribute the prime field elements belonging to a single
extension field element in our vector into adjacent words, we can 
do the shift basically for free by accessing a shifted memory location.
But we still have to deal with the fact that we have $w$ possibly different
highest coefficients $t_{k-1}$ stored together in one word. However,
since we have to multiply all of them with the same prime field element
coming from the expansion of $x^k - c_k = \sum_{i=0}^{k-1} b_i x^i$ 
we can use the method described
for the prime field case above to compute every word to be added to the
shifted vector. That is, for each $k$ adjacent words 
$(a_{jk},a_{jk+1},\ldots,a_{(j+1)k-1})$ in our vector we have to
add the words shifted by one $(0,a_{jk},a_{jk+1},\ldots,a_{(j+1)k-2})$
to the words $(a_{(j+1)k-1} \cdot b_0, \ldots, a_{(j+1)k-1} \cdot b_{i-1})$.
Therefore, the total cost is exactly the same as for one multiplication of
a vector by a prime field scalar and one addition of vectors.

For the full computation of $s \cdot v$, we have to perform this multiplication
by $x$ altogether $k-1$ times, multiply each intermediate result by the
prime field scalar $s_i$ and sum everything up. Thus, the total cost
of the multiplication $s \cdot v$ is at most $2k-1$ multiplications
of a vector by a prime field scalar and $2k-1$ additions of vectors.
Here we count one more addition to keep the formulas a bit simpler
and to account for one copying operation.
Thus, we have proved the following proposition:

\begin{Prop}[Multiplication of vectors by scalars]
\label{multvec}
We assume a machine with $32$ bit word length.
Let $p$ be a prime, $q = p^k$, and $w$ and $e$ as above:
$w = \lfloor 32/e \rfloor$ and $e = \lceil \log_2(2p-1) \rceil$.

For $2 < p < 2^{31}$, the total cost of a multiplication of a compact 
vector $v$ of length $n$ over $\F_q$ by a scalar $s \in \F_p$ is at most 
$14k(e-2)\cdot \lceil n/w \rceil$ word operations (plus memory fetches
and stores). For $p=2$, the scalar
can only be $0$ or $1$ and therefore no computation is necessary at all.

For $2 < p < 2^{31}$, the total cost of a multiplication of a compact
vector $v$ of length $n$ over $\F_q$ by a scalar $s \in \F_q$ is at most
$7k(2k-1)(2e-3)\cdot \lceil n/w \rceil$. For $p = 2$, the total cost is
at most $2k(k-1) \cdot \lceil n/32 \rceil$ word operations.

For a machine with $64$ bit word length the factors $\lceil n/w \rceil$
and $\lceil n/32 \rceil$ have to be changed to $\lceil n/(2w) \rceil$
and $\lceil n/64 \rceil$ respectively
and we can work with primes $p < 2^{63}$.
\end{Prop}

\Proof See above and add up the number of word operations for up
to $2k-1$ multiplications of a vector by a prime field scalar and $2k-1$
additions of vectors. 

This gives the result $7k(2(e-2)+1)(2k-1)\cdot
\lceil n/w \rceil$ for $p > 2$. Note for $p=2$ that the vector $n$ consists of
$k \cdot \lceil n/32 \rceil$ words and that we have to count only one
vector addition for the ``multiplication with a prime field scalar'', since 
the word $a_{(j+1)k-1}$ has to be XORed to those words whose corresponding
bit is set in $x^k - c_k$.
\ProofEnd

\begin{Rem}
Since a vector of length $n$ needs $k \cdot \lceil n/w \rceil$ words
for $p > 2$, this means, that the number of word operations per word of
the vector needed for one multiplication with a scalar is at most 
$7(2k-1)(2e-3)$ for $p > 2$.
\end{Rem}

\subsection{Memory throughput in real implementations}

In this section we present timing results in real implementations. We 
compare two implementations: The first is an implementation of the
ideas presented in this chapter in the {\sf GAP} package {\sf cvec}
(see \cite{cvec}) by the author, and the second is the standard implementation
of compact vectors in the {\sf GAP} kernel written by Steve Linton
(see \cite{GAP4}).
The latter uses byte-oriented table lookup for fields $\F_q$ with 
$3 \le q \le 256$. We compared vector additions over $\F_2$ and $\F_7$, 
and multiplications of vectors by scalars over $\F_7$ in both implementations.
Finally, we tested multiplications of vectors by scalars over $\F_{3^k}$
for $1 \le k \le 5$ in the new implementation.
Note that the {\sf GAP} library does not offer direct access to the
operation $z := v+w$ without memory allocation. Therefore this 
measurement is missing.

We used three relatively new machines with popular microprocessors, 
namely two different machines
with an Intel Pentium 4 with 1024 kB second level cache running at 3.2 GHz, 
and a machine with a AMD Athlon 64 X2 Dual Core Processor 3800+ with 512 kB
second level cache running at 2.0 GHz. The two Pentium 4 machines were
equipped with memory modules with the same specifications (PC 3200, CL3). 

In all runs, no other processes
were consuming significant amounts of CPU time such that nothing should
have interfered with the caches. Strangely enough, the two nearly identical
machines show different performance. 
We cannot explain the differences in memory throughput.
Therefore we have presented both
results to show that such measurements have a certain fluctuation
obviously involving parameters which cannot easily be determined.
For a discussion of these results see below.

To demonstrate the effect of second level caches, we used different 
lengths of vectors. We used vectors using $32 \cdot 10^6$ bytes each in
order to make sure that none of the data is in the second level cache,
and vectors using $125000$ and $62500$ bytes each to make sure that
most accesses should lead to cache hits in the second level cache.
Of course, all operations are repeated many times to get a high accuracy
of the measured time for one operation by averaging.

We tested three different operations. The first, which we denote by 
``$z := v+w$'', is addition of two different vectors and writing the
result to some other memory location. The time for memory allocation
was not considered. The second operation, which we denote by 
``$v := v+w$'', is also addition of two different vectors, but the result
is written into the same memory location as one of the summands.
The third operation is denoted by ``$w := sw$'' and is multiplication
of a vector by a (non-zero) scalar in place, that is, the result overwrites
the original vector. We chose the scalar $6 + 7\Z \in \F_7$, since its
binary expansion is $110$ and it is thus one of the most ``expensive'' scalars
for multiplication. Note that for the case of table lookups the chosen
scalar does not matter.

For vector times scalar computations over $\F_{3^k}$ we always chose as
scalar the coset of a polynomial with no zero coefficients, which is
the worst with respect to performance.

All results are memory throughput values in megabytes per second. 
That is, a result of $1000$ means that altogether $1000$ times
$1024^2$ bytes of memory have been accessed. ``Altogether'' means, that for
the operation $z := v+w$ both read operations (for $v$ and $w$) and the
write operation into $z$ are counted, that is, for vectors of length
$125000$ one such addition needs to access $375000$ bytes in memory,
namely reading $250000$ and writing $125000$. The same holds for
$v := v + w$, whereas the operation $w := sw$ only reads the memory once
and writes it again, which means that one such operation has to access
$250000$ bytes of memory.

All our results are shown in Table~\ref{memthrough}. The columns marked
with ``C'' are results obtained using the {\sf cvec} package and those 
marked with ``L'' are results obtained using the {\sf GAP} library.

In the next section we discuss some aspects of these results.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
Test            & C, P4$_1$ & C, P4$_2$ & C, Ath & 
                  L, P4$_1$  & L, P4$_2$  & L, Ath \\
\hline
\hline
$\F_2$, vectors $32\cdot 10^6 B$, $z := v+w$ 
& 2764 & 2684 & 1940 & ---  & --- & --- \\
\hline                                                    
$\F_2$, vectors $32\cdot 10^6 B$, $v := v+w$ 
& 3629 & 2965 & 2643 & 3577 & 2812 & 2677 \\
\hline                                                    
$\F_2$, vectors $125\,000 B$, $z := v+w$     
& 6839 & 4249 & 8499 & ---  & --- & --- \\
\hline                                                    
$\F_2$, vectors $125\,000 B$, $v := v+w$     
& 6167 & 3791 &12076 & 4818 & 3298 & 12116 \\
\hline                                                    
$\F_2$, vectors $62\,500 B$, $z := v+w$      
& 5643 & 4379 & 7560 & ---  & --- & --- \\
\hline
$\F_2$, vectors $62\,500 B$, $v := v+w$      
& 6458 & 3969 &11237 & 5109 & 3406 &11635 \\
\hline                                                    
\hline
$\F_7$, vectors $32\cdot 10^6 B$, $z := v+w$ 
& 2380 & 1691 & 1902 & ---  & --- & --- \\
\hline
$\F_7$, vectors $32\cdot 10^6 B$, $v := v+w$ 
& 2624 & 1749 & 2515 & 1088 & 708 & 488  \\
\hline                                                    
$\F_7$, vectors $32\cdot 10^6 B$, $w := sw$  
& 2289 & 1504 & 2267 & 1205 & 870 & 422  \\
\hline                                                    
$\F_7$, vectors $125\,000 B$, $z := v+w$     
& 2671 & 1761 & 5745 & ---  & --- & --- \\
\hline
$\F_7$, vectors $125\,000 B$, $v := v+w$     
& 2843 & 1816 & 5356 & 1102 & 713 & 496  \\
\hline                                                    
$\F_7$, vectors $125\,000 B$, $w := sw$      
& 2388 & 1490 & 4273 & 1375 & 876 & 427  \\
\hline                                                    
$\F_7$, vectors $62\,500 B$, $z := v+w$      
& 2765 & 1742 & 5472 & ---  & --- & --- \\
\hline
$\F_7$, vectors $62\,500 B$, $v := v+w$      
& 3080 & 1878 & 5529 & 1170 & 737 & 498 \\
\hline                                                    
$\F_7$, vectors $62\,500 B$, $w := sw$       
& 2386 & 1565 & 5138 & 1342 & 898 & 434 \\
\hline
\hline
$\F_3$, vectors $32\cdot 10^6 B$, $w := sw$
& 2297 & 1471 & 2219 & --- & --- & --- \\
\hline
$\F_9$, vectors $32\cdot 10^6 B$, $w := sw$
& 153 & 98 & 336 & --- & --- & --- \\
\hline
$\F_{27}$, vectors $32\cdot 10^6 B$, $w := sw$
& 99 & 68 & 232 & --- & --- & --- \\
\hline
$\F_{81}$, vectors $32\cdot 10^6 B$, $w := sw$
& 88 & 58 & 204 & --- & --- & --- \\
\hline
$\F_{243}$, vectors $32\cdot 10^6 B$, $w := sw$
& 70 & 47 & 170 & 1166 & 867 & 422 \\
\hline
\end{tabular}
\end{center}
\caption{Memory throughput for vector operations, C: {\sf cvec} package, 
L: {\sf GAP} library}
\label{memthrough}
\end{table}

\subsection{Discussion of results}
\label{ssec:discussion}

The results for the $\F_2$ case show the throughput for raw memory access
since the XOR operations for addition within the processor registers cost
nearly nothing. 

We first look at the top $\F_2$ part of Table~\ref{memthrough}.

We observe that both implementations perform very similarly, which is
only to be expected, since they use exactly the same method. For
main memory access we seem to get nearly identical throughput. For 
second level cache accesses we see some differences, which can be
explained for the given architectures by looking at the C code and the
produced assembler code, which would lead too far here. However, 
the general picture is the same.

One can clearly see in the $\F_2$ part of
Table~\ref{memthrough} that the second level cache seems to help noticeably
when the vectors involved are shorter. This is to be expected, but shows
only a factor of between $1.5$ and $3$ for the Pentium~4 and up to 
a factor of $5$ for the Athlon processor.

Theoretically, one could expect a higher speedup factor, but modern
processors and memory interfaces seem to be highly optimised for 
linear memory accesses using so-called bursts. This means that the
processor has a special method of accessing large amounts of consecutive
memory words consecutively, which we seem to use here.

We turn now to the middle part of the table concerning the field $\F_7$.
Here one can see two important things: The first is that the $7$ word
operations per word stated in Proposition~\ref{addvec} can be done as
fast as main memory access (compare main memory throughput for $p=2$ and
$p=7$ on all architectures). If nearly all accesses lead to hits in 
the second level cache, then the $7$ operations reduce the memory
throughput, but only by a factor of about $2$ to $3$, depending on
the processor architecture.

The second thing is that the byte oriented table lookup is noticeably
slower than the word access. When working in the second level cache,
this can cripple the memory throughput by a factor of up to $10$, at least on
the Athlon architecture. Note that the vector sizes are chosen small
enough, such that not only the three vectors but also the lookup tables
should fit completely in the second level cache together.
The same observation holds for the multiplication with a scalar. 

Of course, for other finite fields $\F_q$ with $q \le 256$ the situation
is different. Among these fields the number $7(2k-1)(2e-3)$ of word
operations per vector word is at most $189$. The experiments shown in
the third part of Table~\ref{memthrough} concerning fields $\F_{3^k}$ 
indicate, that
if this number is assumed, the performance of multiplication of a vector
by a scalar is much slower than with table lookup. 

However, in the light of the grease method explained in
Section~\ref{ssec:vecmat} computing too many vector times scalar 
operations can often be avoided, which renders the new method
superior in many real life examples.

Finally, note that due to the extra bit per prime field element that
is necessary for doing word operations, vectors in the {\cvec}
implementation need a bit more memory than in the {\GAP} library
implementation (for characteristic not equal to $2$).
Thus, if the memory throughput is the same, the practical performance is 
a bit worse with word operations than with table lookup.

Considering all the aspects in this comparison we conclude that there 
is no method that is better in all situations.

\index{Matrix}

\section{Matrix arithmetic}
\label{sec:matarith}

Traditionally, matrices are implemented in the {\sf GAP} system as
lists of (row-) vectors. This means, that the list object only
holds references to the subobjects, which are the rows of the matrix.
Therefore, one can for example permute rows very efficiently without
actually touching the bulk of the data in the rows. Instead, one can
simply redirect references. In addition, different matrices can share
rows, which leads to the fact that the modification of one matrix
can in fact modify the other matrix. Although this can give rise to
nasty bugs, it can also help to increase efficiency both with respect
to memory usage and with respect to performance. We call matrices
implemented with this approach ``row list matrices''.

An alternative way would be to actually embed the row data into the
matrix object itself. A row would then no longer be a {\sf GAP} object
in its own right, but only a part of one. The advantage here is that
we need fewer memory allocations (only one per matrix) and can better
control cache issues, since we know better, where in memory our row data
is stored. We call matrices implemented with this approach ``flat
matrices''.

When devising efficient algorithms dealing with matrices, one has to know
which type of matrix one is using, because one has to know, when the
system actually copies data and when it only passes references.

For the {\cvec} package we chose the row list matrix approach since it fits
better into the existing {\GAP} library and allows for easier code reusage.
We also discuss this approach further in this section. We call a list
of compact vectors of the same length over the same field a ``compact
matrix''.

Note however that Beth Holmes and Richard Parker are currently working
on an implementation of flat matrices in the {\GAP} system.


\subsection{Addition and multiplication with scalars}
\label{addmulsca}

This section is extremely short, since to add matrices or
to multiply a matrix by a scalar simply means to add corresponding
row vectors or multiply all rows by the scalar respectively.
Thus we immediately get from Propositions~\ref{addvec} and
\ref{multvec}:

\begin{Cor}[Matrix addition and multiplication of matrices by scalars]
We assume a machine with $32$ bit word length. Let $M$ and $M'$ be 
two compact matrices with $m$ rows and $n$ columns over $\F_q$, where $q
= p^k$.

For\/ $2 < p < 2^{31}$, the matrices $M$ and $M'$
can be added using $7km\cdot \lceil n/w \rceil$
word operations
(plus memory fetches and stores), where $w = \lfloor 32/e \rfloor$
and $e = \lceil \log_2(2p-1) \rceil$. 
For $p=2$, only $mk \cdot \lceil n/32 \rceil$ word operations are needed.

For\/ $2 < p < 2^{31}$, the total cost of a multiplication of
$M$ by $s \in \F_p$ is at most 
$14km(e-2)\cdot \lceil 32/w \rceil$ word operations (plus memory fetches
and stores). For $p=2$, the scalar
can only be $0$ or $1$ and therefore no computation is necessary at all.
For\/ $2 < p < 2^{31}$, the total cost of a multiplication of $M$
by $s \in \F_q$ is at most
$7k(2k-1)m(2e-3)\cdot \lceil n/w \rceil$.
 For $p = 2$, the total cost is
at most $2k(k-1)m \cdot \lceil n/32 \rceil$ word operations.

For a machine with $64$ bit word length the factors $\lceil n/w \rceil$
and $\lceil n/32 \rceil$ have to be changed to $\lceil n/(2w) \rceil$
and $\lceil n/64 \rceil$ respectively
and we can work with primes $p < 2^{63}$.
\end{Cor}
\Proof This is immediate from Propositions~\ref{addvec} and \ref{multvec}.
\ProofEnd

\subsection{Vector-matrix multiplication}
\label{ssec:vecmat}

This section is about the multiplication of a row vector 
$v \in \F_q^{1 \times m}$ by a matrix $M \in \F_q^{m \times n}$. The result
is a vector $vM \in \F_q^{1 \times n}$. Since by convention we are working 
mainly with row vectors, this is the only operation between vectors
and matrices we have to consider. The left multiplication of a column
vector by a matrix can be simulated using the transposed matrix.

The multiplication $vM$ can be understood as taking a linear combination
of the rows of $M$, the vector $v$ contains the coefficients. This
kind of reasoning already leads to the most efficient way to implement
this operation: We multiply the $i$-th row of $M$ by the scalar
$v_i$ and add the resulting vector to the final result for $1 \le i \le m$.
Thus this operation can be done using at most $m$ multiplications
of a vector by a scalar plus $m-1$ vector additions.

If we are considering only one vector-matrix multiplication this is basically
all one can say. However, in a situation in which we apply one given
matrix repeatedly to lots of vectors (for example during matrix 
multiplication (see below) or when enumerating orbits), there is a
trick called ``grease'', which was invented by Richard Parker and 
others (see for example \cite[Algorithm 6.2]{ahohop}).

The idea is that if the base field is small, there are not so many
different linear combinations of a finite number of vectors. Thus,
we can distribute the rows of our matrix into small groups, say of
$\ell$ adjacent rows each, and precompute all possible linear combinations of
all vectors in each group. If we now want to multiply a vector $v$ and the 
matrix, we extract the entries of $v$ corresponding to each group,
look up the linear combination and add it to the result. The number
$\ell$ of elements in each group is called ``grease level''.

\begin{figure}[ht]
\begin{center}
\input{grease.pstex_t}
\end{center}
\caption{Illustration for Vector-matrix multiplication and grease}
\label{grease}
\end{figure}

This technique is illustrated pictorially in Figure~\ref{grease}. There
one can see a vector-matrix multiplication. The vector is divided into
sections of length $\ell$, beginning from the left, leaving a section of
possibly less than $\ell$ elements at the right hand side. Each such section
corresponds to $\ell$ (or less at the end) adjacent rows in the matrix, we 
call such a submatrix a ``grease block''.
To do grease level $\ell$, we compute all possible linear combinations of
each block of $\ell$ rows and store them. If we have computed and stored
all this data, we call the matrix ``greased''.
Then a vector-matrix multiplication
with this matrix only has to perform $\lceil m/\ell \rceil -1$ vector additions
of looked up vectors. More precisely, we have the following result:

\begin{Theo}[Grease: expected cost and gain]
\label{theogrease}
Let $M$ be a matrix with $m$ rows and $n$ columns over the field\/ $\F_q$
for $q = p^k$, and let $\ell \in \Z_{> 0}$.

Then greasing the matrix $M$ with grease level $\ell$ multiplies the amount
of memory needed for $M$ by at most $q^\ell$. The precomputation of the
linear combinations costs at most 
$q^\ell-k\cdot \ell -1$ row vector additions 
and $\ell \cdot (k-1)$ multiplications of a row vector with a primitive
root of\/ $\F_q$ per grease block. That is, for the complete matrix, we
need at most $\lceil m/\ell \rceil \cdot (q^\ell-k\cdot \ell -1)$ row
vector additions and $m \cdot (k-1)$ multiplications by the primitive
root.

If $M$ is greased, a vector $v$ can be multiplied from the right by $M$
using at most $\lceil m/\ell \rceil$ row vector additions plus the 
cost of extracting the finite field elements from the vector $v$, which
is usually negligible.

The standard approach for a vector-matrix multiplication needs 
approximately $m(q-1)/q$ multiplications of a row vector by a non-zero
scalar from\/ $\F_q$ plus $(m-1)(q-1)/q$ row vector additions, assuming that 
about every $q$-th element of $v$ is equal to zero.
\end{Theo}
\Proof We first discuss the precomputation step to compute the greased
matrix. Since there are $q^\ell$ different linear combinations of $\ell$ 
vectors the statement about the needed memory is clear.

We have to do the following for every grease block and thus 
all costs are multiplied by $\lceil m/\ell \rceil$. 
After multiplying each vector in the
block $k-1$ times by the primitive root of $\F_q$ represented by the
polynomial $x \in \F_p[x]$ and storing the intermediate results, 
we can compute all $\F_q$-linear combinations
of the vectors in the block by computing all $\F_p$-linear combinations
of the vectors we already have. Thus, by Lemma~\ref{alllinkomb} below we can
compute all those linear combinations with 
$p^{k\cdot \ell} - k\cdot \ell - 1 = 
q^\ell - k \cdot \ell - 1$ row vector additions, which is the cost in the
theorem. Note that the scalar multiplications by the
primitive root of\/ $\F_q$ can be done
more efficiently since the element represented by the polynomial $x$
is sparse, such that the scalar multiplication by it is faster.

After having greased the matrix, a multiplication of a vector by 
the matrix can be done by adding appropriate vectors. We have to add
one for every grease block, leading to $\lceil m/\ell \rceil - 1$ additions.
However, since usually the result has to be stored in a different location
than the table of greased vectors, we have to copy the first summand
somewhere. Thus we count one addition to account for this copying operation.
\ProofEnd

\begin{Rems}
\begin{itemize}
\item The factor $(q-1)/q$ used for the standard approach is a
good estimate for practical considerations when comparing to
the grease approach. For larger values of $q$ it is of course negligible,
however for $q=2$ it is equal to $1/2$.
\item It is crucial to implement the extraction of entries
from the vector $v$ as efficient as possible. If this is not done
right, the cost of extracting elements in one vector can cancel out
the benefit of fewer scalar multiplications.
\item The fact that we need very few scalar multiples in the
precomputation phase and none in the vector times matrix phase is
very useful with respect to our new implementation in the
{\cvec} package, since there an addition is sometimes much cheaper
than a multiplication with a scalar.
\end{itemize}
\end{Rems}
 
The following lemma is used in the proof of Theorem~\ref{theogrease}:

\begin{Lemm}[Computing all\/ $\F_p$-linear combinations]
\label{alllinkomb}
Let $v_1, \ldots, v_{j}$ be\/ $\F_p$-linearly  
independent vectors over a finite field\/ $\F_q$ with $q = p^k$
for some $j \in \Z_{> 0}$. If
all\/ $\F_p$-linear combinations of $v_1, \ldots, v_{j-1}$ are already
computed, then all\/ $\F_p$-linear combinations of $v_1, \ldots, v_j$ can
be computed with $p^j - p^{j-1} - 1$ vector additions.

This implies in particular that all\/ $\F_p$-scalar multiples of a vector
can be computed with $p-2$ vector additions, and inductively, that
all\/ $\F_p$-linear combinations of $v_1, \ldots, v_j$ can be computed
without assumption using $p^j-j-1$ vector additions.
\end{Lemm}
\Proof All non-zero $\F_p$-scalar multiples of $v_j$ can be computed by
successively adding $v_j$ altogether $p-2$ times. Then every $\F_p$-linear
combination of $v_1, \ldots, v_j$ is either one of the already
computed $\F_p$-linear combinations of $v_1, \ldots, v_{j-1}$ or the
non-zero multiples of $v_j$, or a sum of two such vectors. Thus, we 
can compute all missing $p^j - p^{j-1} - (p-1)$ vectors with one
vector addition each, which proves the first statement. 

This includes the statement for $j=1$, which handles the case of computing
all $\F_p$-scalar multiples of one vector. Without assumptions, we can
apply the first statement inductively for $i=1, 2, \ldots, j$ proving
the last statement, since
\[ \sum_{i=1}^j (p^i - p^{i-1} - 1) = p^j - j - 1. \]
\ProofEnd

\begin{Rem}
Computing all $\F_p$-linear combinations can be done
with one addition per vector that is not already given, assuming
the zero vector and the input vectors are already there.
\end{Rem}

\subsection{Matrix multiplication}

Let $M \in \F_q^{m \times n}$ and $N \in \F_q^{n \times s}$ with $q = p^k$.
The matrix multiplication $M \cdot N$ can be computed by multiplying the
rows of $M$ from the right with the matrix $N$ and putting the $m$ resulting
rows of length $s$ into the resulting matrix in $\F_q^{m \times s}$.

There are at least three methods to improve the performance of this
computation: The first is using grease (see the previous section), which
we will analyse in detail in this section. 

The second is arranging the same computations in a different order to 
achieve that more memory fetches and stores are handled by the second
level cache. This approach is used in some implementations and can
speed up computations. Since the third method produces this cache locality
automatically, we only come back to this below briefly.
For some additional discussion of this topic see Section~\ref{sec:cache}. 

The third method is to use the methods of Strassen and Winograd to
get a lower exponent than $3$ in the complexity of matrix multiplication
of square matrices.

We now discuss grease before we analyse for which sizes of
matrices the Strassen/Winograd method is worthwhile doing.

When the matrices we want to multiply are big enough such that the number
of rows of $M$ is large in comparison to $q^\ell$
for a suitable chosen grease level $\ell$, it is not necessary to grease
the complete matrix $N$ before beginning the multiplication. Rather,
we can proceed with the whole multiplication ``by grease blocks'', that
is, we compute in a first step all possible linear combinations of the
first $\ell$ rows of $N$, then run through all rows of $M$ looking
only at the first $\ell$ entries, look up the right
linear combination of the first $\ell$ rows of $N$ and add it 
to the corresponding
row of the final result. After that we can forget our precomputed linear
combinations and proceed to the next grease block in $N$ and so on.

We have to determine the appropriate grease level (where $0$ stands for not
using grease at all). To this end we 
compare $(q^\ell/\ell - k - 1/\ell) + m/\ell$ 
row vector additions plus $k-1$ row vector multiplications to
$m \cdot (q-1)/q$ row vector multiplications and additions.
By Theorem~\ref{theogrease} the former are the numbers of row vector
operations necessary for the grease approach and the latter the number
of row vector operations for the standard approach, both counted for
one column of the left matrix. Obviously, grease becomes more interesting 
when $m$ is large in comparison to $q^\ell$. 
Furthermore, the advantage of grease
grows with the matrix size.

To optimise this, a calibration phase is done once and for all: For all finite
fields separately, a procedure determines, how long a multiplication
of a vector by an arbitrary scalar and the primitive root respectively
takes in comparison to an addition of two vectors.
This depends on the particular machine architecture, especially on 
cache sizes and memory interfaces and thus has to be done for every
machine separately. The result is a table advising which level of grease
to use for which values of $m$. Since all the above mentioned cost
functions are linear functions in $m$, it is easy to show that the
optimal grease level depends monotonically increasing on $m$.

Note that this analysis shows that in particular for values of $q$
for which multiplication of row vectors by scalars is expensive (see
Section~\ref{addmulsca}) even grease level $\ell = 1$ is worthwhile
doing.

When we use the optimal grease level determined as described, the
cost of matrix multiplication depends on the size of the matrices
in a relatively complicated way. In Figures~\ref{matmulgrease} and
\ref{matmulgrease_scaled} we show times $t$ of matrix multiplications
over $\F_2$ for different sizes $n$ of square matrices using grease.
The timings were obtained by using the {\cvec} package in {\GAP} on a
machine with an AMD Athlon 64 X2 Dual Core processor 4600+ with 512
kB second level cache running at 2.4 GHz. The displayed behaviour is
typical and holds also for other finite fields.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{matmulF2grease}
\end{center}
\caption{Multiplication times over $\F_2$ against matrix dimension
using grease}
\label{matmulgrease}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{matmulF2grease_scaled}
\end{center}
\caption{Multiplication times over $\F_2$ against matrix dimension
using grease
(magnification)}
\label{matmulgrease_scaled}
\end{figure}

The actual measurements are depicted by little $\times$ signs and
the curve is a plot of the function $t = cn^3$ for some constant $c$, such
that the curve passes through the actual measurement for $n=5000$.

We see two effects. The first is that the actual measurements do not lie
exactly on the theoretical $n^3$ curve. Rather, most measurements lie above
that curve. This means that multiplying small matrices seems to take
longer than expected by the needed number of operations in comparison 
to larger matrices. This effect can be explained on the one hand by grease,
which is more effective the bigger the matrices become. On the other hand
for larger matrices the actual row operations dominate the administrative
tasks like loops and also memory accesses to large consecutive areas of
memory seem to be more efficient.

The second effect is that for very small matrices (smaller than about $1500
\times 1500$ for the field $\F_2$) the multiplication is again faster
than expected. This is due to the second level cache built into nowadays
microprocessors. A $(1500\times 1500)$-matrix over $\F_2$ needs about
$300$ kB of memory. Thus, for smaller matrix multiplications most memory
accesses are in fact cache hits.

For really large values of $m$ and fixed $\ell$ we can neglect the cost for the
precomputation step of greasing and thus the total number of row 
vector additions for the matrix multiplication of $M$ by $N$ is
approximately $m \cdot \lceil n/\ell \rceil$. Note that we cannot
neglect the cost for the precomputation for larger values of $\ell$,
since the precomputation cost contain a term $q^\ell$. Thus in practice,
the grease level always stays rather small.

Since the cost of
a row vector addition of vectors of length $s$ is proportional
to $s$ (compare Proposition~\ref{addvec}), we get an asymptotic upper
bound for the total cost for matrix multiplication that is proportional
to $mnsk$ (remember that we fixed $p < 2^{31}$ or $p< 2^{63}$ in this
analysis). For square $(n \times n)$-matrices this is the classical
$O(n^3)$ complexity for the number of field operations.

We now discuss methods by Strassen and Winograd to reduce this complexity
for large matrices. These methods apply in practice only to square matrices.
The basic idea is that it is possible to multiply two $(2 \times 2)$-matrices
using only $7$ multiplications instead of $8$ in the standard approach.
One pays for this with a few more additions. Using this idea for two 
$(2n \times 2n)$-matrices means cutting both into $4$ blocks of 
$(n\times n)$-matrices each and computing the whole product by computing
$7$ matrix products of $(n \times n)$-matrices. For details of the
procedures developed by Strassen and Winograd see \cite[4.6.4]{AOCP2}.
With their help one can reduce the theoretical complexity for the 
multiplication of two $(n \times n)$-matrices from $O(n^3)$
to $O(n^{\log_2(7)}) \approx O(n^{2.807})$. 
There are also more involved methods reducing the exponent
further, but they seem to be of theoretical interest only (see 
also \cite[4.6.4]{AOCP2}).

Using the Winograd method recursively needs a certain amount of
additional administration, mostly cutting bigger matrices into smaller
ones and copying the smaller intermediate results back into the
bigger result. Since the cost of all this administration is proportional
to the area of the matrices involved (i.e.~it is only quadratic in the
size of the matrices), it is clear that this will be worthwhile for 
``large'' matrices. For smaller matrices, the administrative overhead
might dominate the saving effect of the better complexity.
In particular, the method is only good if $7$ multiplications of two $(n
\times n)$-matrices plus the administrational overhead need in fact less
time than one multiplication of two $(2n \times 2n)$-matrices.

The percentage of administrative work for the preparation of a matrix
multiplication among the total work is higher for small matrices.
This fact and the increasing effect of grease lead to the
fact that for matrices over finite fields $n$ needs to be rather large
for the above condition to be fulfilled. 

Therefore, one has to test individually for each finite field and each
machine type, from which limit for $n$ on the Winograd method should
be used. It seems that one can safely assume that provided it is 
worthwhile for some $n_0$, it is also useful for all $n > n_0$. 

However, in a practical implementation things are not that easy. In
the following Table~\ref{MatMulWinograd2} we have collected empirical
data about runtimes of matrix multiplication for different sizes of
square matrices over the field $\F_2$. In the different columns we vary
the number of times we use the Winograd trick. ``Wino 2'' for example
means that the trick is used twice, that is, the matrices are cut once
and the pieces are cut once again.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|r||r|r|r|r|}
\hline
$n$ & Wino 0 & Wino 1 & Wino 2 & Wino 3 \\
\hline
1500 & \textbf{36}     &66   & 152    &395      \\
2000 & 125    &\textbf{116}  & 242    &600      \\
2500 & 218    &\textbf{190}  & 360    &840      \\
3000 & 350    &\textbf{310}  & 505    &1090      \\
3500 & \textbf{520}    &600  & 670    &1410      \\
4000 & \textbf{790}    &960  & 870    &1790      \\
4500 & \textbf{1070}   &1270 & 1130   &2180      \\
5000 & 1480   &1670 & \textbf{1420}   &2640      \\
5500 & 1930   &2070 & \textbf{1720}   &3130      \\
6000 & 2540   &2610 & \textbf{2190}   &3680      \\
6500 & \textbf{3090}   &3170 & 3460   &4240      \\
7000 & \textbf{3810}   &3870 & 4670   &4930      \\
7500 & 4760   &\textbf{4670} & 5630   &5580      \\
8000 & 5700   &\textbf{5600} & 6800   &6280      \\
8500 & \textbf{6700}   &6720 & 8150   &7160      \\
9000 & 8050   &\textbf{7910} & 9130   &8220      \\
9500 & 9310   &\textbf{9070} & 10400  &9130      \\
10000 &10800  &11430&  11920 & \textbf{10170}     \\
\hline
\end{tabular}
\end{center}
\caption{Matrix multiplication times over $\F_2$ using Winograd}
\label{MatMulWinograd2}
\end{table}

One can observe that it is not true that the bigger the matrices are
the more often one should use the Winograd trick. For $(4000 \times
4000)$-matrices it is for example better not to use the Winograd trick
at all, whereas it helps to do it once for $(3000 \times
3000)$-matrices. Even worse, if the best number of
Winograd applications for $(n \times n)$-matrices is $i$, then it 
is not necessarily true that the best number of Winograd applications
for $(2n \times 2n)$-matrices is $i+1$ (see for example $n=2000$ in 
Table~\ref{MatMulWinograd2}). The situation for the base field 
$\F_2$ is admittedly an extreme example but it is typical for small
fields as can be seen by comparison with the field $\F_3$ for which we
show some analogous timings in Table~\ref{MatMulWinograd3}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|r||r|r|r|r|}
\hline
$n$ & Wino 0 & Wino 1 & Wino 2 & Wino 3 \\
\hline
1500 & 425 & \textbf{415} & 510 & 800\\
2000 & 980 & \textbf{940} & 1020 & 1420\\
2500 & \textbf{1810} & 1880 & 1870 & 2400\\
3000 & 2970 & 3100 & \textbf{2900} & 3660\\
3500 & 4650 & 4760 & \textbf{4420} & 5350\\
4000 & \textbf{6620} & 6850 & 6690 & 7310\\
4500 & \textbf{9260} & 9820 & 9840 & 9970\\
5000 & \textbf{12420} & 12910 & 13250 & 13280\\
5500 & \textbf{16340} & 16740 & 17260 & 16770\\
6000 & 20800 & 20840 & 21780 & \textbf{20760}\\
6500 & 26350 & 26370 & 27370 & \textbf{25920}\\
7000 & 31990 & 32590 & 33470 & \textbf{31210}\\
7500 & 39730 & 39350 & 41290 & \textbf{39260}\\
8000 & 47430 & \textbf{46610} & 48350 & 47040\\
8500 & 57960 & \textbf{55610} & 57580 & 59040\\
9000 & 69450 & \textbf{65310} & 69100 & 69410\\
9500 & 78740 & \textbf{75780} & 79880 & 81380\\
10000 & 91130 & \textbf{87290} & 91120 & 93040\\
\hline
\end{tabular}
\end{center}
\caption{Matrix multiplication times over $\F_3$ using Winograd}
\label{MatMulWinograd3}
\end{table}

A good theoretical explanation of these findings is difficult, in
particular since such measurements depend very much on the particular
processor type and memory interface. As mentioned above, at least
three factors
seem to play a role: The first is that the saving effects of grease
are greater for bigger matrices, the second is that administrative
costs for the Winograd method play a bigger role for smaller matrices
and finally the processor caches and specialities in main memory
interfaces like burst accesses sometimes have nearly unpredictable
effects on performance. For example for the field $\F_2$ it seems that
for square matrices in the range of $n=3500$ to $n=4500$ cutting them
once does not make the pieces small enough to fit into the second
level cache and doing the Winograd trick twice already involves
too much administrative overhead such that altogether it is best not
to use the Winograd method at all. For smaller matrices in the range $n=2000$
to $n=3000$ on the other hand it seems worthwhile to use Winograd once
which makes the pieces fit into the second level cache. For $n=5000$ in
turn it seems to be the best to use Winograd twice to end up with
matrices of size $1250$ fitting into the second level cache. Thus it
seems that this strange cache related behaviour in the range 
$2000$--$4500$ is repeated in the range $5000$--$9000$ and possibly
above.

% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=14cm]{matmulF2_all}
% \end{center}
% \caption{Matrix multiplication times over $\F_2$ against matrix dimension}
% \label{matmulf2}
% \end{figure}
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=14cm]{matmulF2_low}
% \end{center}
% \caption{Matrix multiplication times over $\F_2$ against matrix dimension
% (magnification)}
% \label{matmulf2scaled}
% \end{figure}
% 
% We want to underpin this claim by experimental evidence. In
% Figures~\ref{matmulf2} through \ref{matmulf7scaled} we have depicted times
% for the multiplication of two randomised $(n\times n)$-matrices, not
% using the Strassen/Winograd method but using grease.
% All measurements were performed on a machine with a AMD Athlon 64 X2 
% Dual Core Processor 4600+ with 512 kB second level cache running at 2.4 GHz.
% 
% The horizontal axis indicates the size $n$ of the matrices and the vertical
% axis the time $t$ for one multiplication in milliseconds. 
% The actual measured values are depicted by
% little $\times$ signs. A curve with equation $t=c \cdot n^3$ with some constant
% $c$ has been fitted to the origin and the measurement for the biggest 
% matrices. 
% 
% One can see that the measurements are very near the expected
% curve but a little bit above. This is due to the fact that administrative
% saving effects and grease render the multiplication
% of bigger matrices a little more efficient. 
% 
% In addition for every tested matrix size we have added two further marks.
% Namely, for a measurement $(n,t)$ we have put a little $+$ sign at the
% position $(2n,7t)$ and a little square sign at the position $(2n,8t)$.
% In this way, one can easily compare the time for one multiplication of two
% $(2n \times 2n)$-matrices with the time for 7 or 8 multiplications of two
% $(n \times n)$-matrices respectively. The figures clearly show, that the $\times$
% sign is closer to the square sign than to the $+$ sign except for rather
% large matrices.
% 
% For the field $\F_2$ with $2$ elements for example and for $2n$ in the range 
% from $3000$ to about $10000$ there seems to be no point in using the 
% Strassen/Winograd method at all, 
% since the time for one multiplication of two $(2n \times 2n)$-matrices
% is closer to that of 7 multiplications of two $(n \times n)$-matrices than to
% that of 8. For bigger values of $2n$, this method is definitely
% sensible since one can save about $1/8$ of the computing time by
% doing Winograd once. However, even for smaller matrices the method
% helps, since it can help to ...
% 
% And indeed, experiments with our Winograd-implementation give
% further evidence, that using this method is only worthwhile over the field
% $\F_2$ for matrices of at least size $1600 \times 1600$.
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=14cm]{matmulF7_all}
% \end{center}
% \caption{Matrix multiplication times over $\F_7$ against matrix dimension}
% \label{matmulf7}
% \end{figure}
% 
% For the field with $7$ elements the situation is similar,
% Figures~\ref{matmulf7} and \ref{matmulf7scaled} show the details
% analogously. Here the break-even point lies at about $600$.
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=14cm]{matmulF7_low}
% \end{center}
% \caption{Matrix multiplication times over $\F_7$ against matrix dimension
% (magnification)}
% \label{matmulf7scaled}
% \end{figure}
% 
% For the field with $3^5 = 243$ elements the situation is even more extreme.
% Figure~\ref{matmulF243} shows the details analogously. The saving effects
% of grease are so prominent that it is never a good idea to use the Winograd 
% method for matrices that fit into the main memory of today's computers.
% Note that a matrix of dimensions $15000 \times 15000$ over $\F_{243}$
% needs about $450$ megabytes in main memory.
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=14cm]{matmulF243_all}
% \end{center}
% \caption{Matrix multiplication times over $\F_{243}$ against matrix dimension}
% \label{matmulF243}
% \end{figure}
% 
% The behaviour for other finite fields is similar. 
The {\cvec} package
contains an implementation of the Winograd method and uses a calibration
phase to determine for every finite field with $q \le 1024$ from which
size the method is worth doing. In fact, there are two implementations
of this method, one using more memory and less administrative overhead
and one using less memory. The first is used for smaller matrices since
for them it is crucial to cut down administrative costs as much as
possible. For really large matrices, the extra memory needed would hurt
too much and the saving effects of the better complexity render the
method useful even with the higher administrative overhead.

However, the question of when to use the Winograd method and how often
in practical applications remains quite difficult and highly machine
dependent.

%The following table shows for some finite fields the size limits for
%square matrices, from which the {\cvec} package uses the Winograd method.
%Note that these numbers depend heavily on the machine architecture and
%thus have to be determined individually for every machine.
%
%\begin{center}
%\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
%\hline
%q & 2 & 3 & 4 & 5 & 7 & 8 & 9 & 16 & 17 & 47 \\
%\hline
%n & 1600 & 1430 & 1502 & 1137 & 1438 & 1186 & 1201 & 1151 & --- & 800 \\
%\hline
%\end{tabular}
%\end{center}
%
%The dash for $q=17$ means that the Winograd method is not used at all 
%for matrices over the field with $17$ elements.

We conclude this section with Figures~\ref{matmulf2} and
\ref{matmulf3} showing the
performance of matrix multiplication using grease together with
the Winograd method. Here, we used the Winograd method for matrices
of size greater or equal to $n=2500$ for $\F_2$ and ($n=3000$ for
$\F_3$), that is, from $n=5000$ ($n=6000$ resp.) on it
is used twice and so on. The curve shown there is 
again the plot of the function $t = cn^3$ for some constant $c$, such
that the curve passes through the actual measurement for $n=15000$
($n=10000$ resp.).
The better complexity begins to show with a few outliers following
the pattern observed above.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{matmulF2_all}
\end{center}
\caption{Matrix multiplication times over $\F_2$ against matrix dimension}
\label{matmulf2}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=14cm]{matmulF3_all}
\end{center}
\caption{Matrix multiplication times over $\F_3$ against matrix dimension}
\label{matmulf3}
\end{figure}

% \subsection{Semi echelonisation}
% 
% The concept of echelonisation is fundamental to solve systems of linear
% equations (see Proposition~\ref{syslineqsolve}). All the concepts
% introduced in this section are well known. We present them here for
% the sake of completeness and to present a complete analysis in terms
% of numbers of row vector operations.
% 
% \begin{Def}[Row echelon form]
% Let $\F$ be an arbitrary field. 
% 
% We say that a matrix $S = (S_{i,j}) \in \F$
% is \textbf{in row semi echelon form}, if the first non-zero entry in each
% row is equal to $1$ and every entry in the same column below these ones
% is equal to $0$. More formally, the condition is: There
% is a non-negative integer $r \le m$ and integers $1 \le j_1 \le \cdots \le
% j_r \le n$ such that the following hold:
% \begin{itemize}
% \item $S_{i,j_i} = 1$ and $S_{i,k} = 0$ for all $i$ with $1 \le i \le r$
% and for all $k$ with $1 \le k < j_i$,
% \item $S_{k,j_i} = 0$ for all $i$ with $1 \le i \le r$ and all $k$ with 
% $j_i < k \le m$,
% \item $S_{k,l} = 0$ for all $k$ with $r < k \le m$ and for all $l$ with
% $1 \le l \le n$.
% \end{itemize}
% Note that the numbers $r$ and $j_i$ are --- if they exist --- uniquely
% determined by the matrix $S$. The number $r$ is the rank if $S$ and
% the $j_i$ are called \textbf{pivot column numbers} and the corresponding
% columns \textbf{pivot columns}. 
% 
% We say that a matrix $S = (S_{i,j}) \in \F$
% is \textbf{in row full echelon form}, if the first non-zero entry in each
% row is equal to $1$ and every entry in the same column below and above
% these ones is equal to $0$. More formally, the condition is: There
% is a non-negative integer $r \le m$ and integers $1 \le j_1 \le \cdots \le
% j_r \le n$ such that the following hold:
% \begin{itemize}
% \item $S_{i,j_i} = 1$ and $S_{i,k} = 0$ for all $i$ with $1 \le i \le r$
% and for all $k$ with $1 \le k < j_i$,
% \item $S_{k,j_i} = 0$ for all $i$ with $1 \le i \le r$ and all $k$ with 
% $1 \le k < j_i$ or $j_i < k \le m$,
% \item $S_{k,l} = 0$ for all $k$ with $r < k \le m$ and for all $l$ with
% $1 \le l \le n$.
% \end{itemize}
% Every matrix in row full echelon form is in particular in row semi echelon form.
% Sometimes we leave out the word ``row'' and speak of semi echelon form
% or full echelon form. \ProofEnd
% \end{Def}
% 
% The reason why a matrix $S \in \F^{m \times n}$ in row semi echelon     
% form is useful is that it defines the direct sum decomposition          
% $V := \F^{1 \times n} = \Span(S) \oplus \Cmpl(S)$ where $\Span(S)$
% is the linear span of the rows of $S$
% and $\Cmpl(S) := \left\{ v \in V \mid v_{j_i} = 0 \mbox{ for } 1 \le i \le r
% \right\}$ and that there are very efficient
% algorithms to deal with this decomposition. It is for example possible
% to compute the canonical projections $V \to \Span(S)$ and 
% $V \to \Cmpl(S)$ and thus
% test membership in $\Span(S)$ and $\Cmpl(S)$, or explicitly decompose a 
% vector $v \in V$
% info $v=u+w$ with $u \in \Span(S)$ and $w \in \Cmpl(S)$. It is also possible to
% express an arbitrary vector $u \in \Span(S)$ in the form $u = a\cdot S$ with
% $a \in \F^{m \times 1}$ which essentially solves a system of linear
% equations given by $S$.
% 
% Since all this is well known from any linear algebra course, we only 
% describe and analyse the one fundamental Algorithm \textsf{CleanRow}
% before we move on to algorithms to transform arbitrary matrices to
% echelon form.
% 
% \begin{figure}[ht]
% \hrule
% \vspace*{1mm}
% \begin{algorithmic}
% \STATE \textbf{Input:} $S \in \F^{m \times n}$ in
% row semi echelon form with $r$ and $1 \le j_1 \le \cdots \le j_r \le m$,
% and $v \in \F^{1 \times n}$
% \STATE \textbf{Output:} $w \in \Cmpl{S}$ with $v-w \in
% \Span(S)$ and $a \in \F^{1 \times m}$ with $a\cdot S = v-w$
% \vspace*{1mm}
% \STATE $w := v$
% \STATE $a := 0 \in \F^m$, or $a:=\emptyset$ if $m=0$ \hspace*{2.4mm}
% \COMMENT{note that $w=v-aS$ if $m\ne0$}
% \FOR {$i = 1$ to $r$}
%     \STATE $a_i := w_{j_i}$
%     \STATE $w := w - a_i \cdot S[i]$
%     \hspace*{2cm} \COMMENT{here $S[i]$ is the $i$-th row of $S$}
% \ENDFOR\hspace*{4.2cm}
% \COMMENT{still $w=v-aS$ if $m\ne0$}
% \STATE \textbf{return} $w,a$
% \end{algorithmic}
% \vspace*{1mm}
% \hrule
% \caption{Algorithm \textsf{CleanRow}}
% \label{AlgCleanRow}
% \end{figure}
% 
% \begin{Prop}[Analysis of Algorithm~\ref{AlgCleanRow} \textsf{CleanRow}]
% Given $S \in \F^{m \times n}$ in row semi echelon form and $v \in \F^{1
% \times n}$, Algorithm~\ref{AlgCleanRow} \textsf{CleanRow} returns
% vectors $w \in \F^{1 \times n}$ and $a \in \F^{1 \times m}$ such that
% $aS + w = v$ and $w \in \Cmpl(S)$. Thus, it computes the decomposition
% of $v$ with respect to the direct sum 
% $\F^{1 \times n} = \Span(S) \oplus \Cmpl(S)$.
% 
% The algorithm needs at most $r$ multiplications of a row vector in\/ $\F^{1
% \times n}$ by a scalar and $r$ additions of two such row vectors.
% \end{Prop}
% \Proof After step $i$ in the loop the vector $w$ has zeroes
% in positions $j_1, \ldots, j_i$, because $S$ is in row semi echelon form.
% In particular, the zeroes below the ones in the pivot columns see to it,
% that the zeroes in $w$ are not changed any more in later steps of the loop.
% 
% The statement about the costs is obvious.
% \ProofEnd
% 
% \smallskip
% Algorithm~\ref{AlgCleanRow} \textsf{CleanRow} already enables us to perform
% the following tasks: It computes the canonical projection of $v$ onto
% $\Cmpl(S)$ with respect to the decomposition\/ $\F^{1 \times n} = \Span(S)
% \oplus \Cmpl(S)$. From this we can compute the other canonical
% projection of $v$ onto $\Span(S)$ by one more row operation $v-w$.
% In particular, we thus can test membership in $\Span(S)$ and $\Cmpl(S)$
% by comparing $w$ or $v-w$ to zero respectively. Also, for $v \in \Span(S)$
% we can solve the system of linear equations $v = xS$, the unique solution
% is $x=a$.
% 
% To solve more generally a system of linear equations $v=aS$ for an
% arbitrary matrix $M \in \F^{m \times n}$ we want a way to apply
% these results. Given $M$, the basic idea is to find a matrix
% $S \in \F^{r \times n}$ in row semi echelon form together with a
% lower triangular matrix $T \in \F^{r \times m}$ with $S = TM$, where $r$
% is the rank of $M$ and $\Span(S) = \Span(M)$.
% 
% This can be achieved by starting with a empty matrices 
% $S \in \F^{0 \times n}$ and $T \in \F^{0 \times m}$ and successively
% using Algorithm~\ref{AlgCleanRow} to clean the rows of $M$. We describe
% one such step in detail here. 
% 
% Assume inductively, that 
% $S \in \F^{k \times n}$ is in row semi echelon form,
% $T \in \F^{k \times m}$ with $S = TM$, and $v$ is a row of $M$.
% Let $w$ and $a$ be the results of Algorithm~\ref{AlgCleanRow} called
% with $S$ and $v$. If $w = 0$ then $v \in \Span(S)$ and we proceed to the
% next row of $M$ without changing $S$ and $T$. Otherwise, $w \in \Cmpl(S)$
% and by multiplying $w$ with a non-zero constant $c \in \F$ we get
% a vector $cw$ whose first non-zero entry is equal to $1$. We append
% $cw$ to the matrix $S$ and get $S' \in \F^{(k+1) \times n}$,
% again in row semi echelon form.
% Since $cw = c\cdot (v-aS) = cv - caTM$, appending the row 
% $(-caT,c,0,\ldots,0) \in \F^{1 \times m}$ to $T$ gives us a new 
% $T' \in \F^{(k+1)\times m}$ with $S' = T'M$.
% 
% This procedure inductively solves the above problem. We summarise:
% 
% \begin{Prop}[Semi echelonisation]
% Let $\F$ be an arbitrary field and $M \in \F^{m \times n}$. Then the
% above inductive procedure computes a matrix $S \in \F^{r \times n}$
% in row semi echelon form and a lower triangular matrix 
% $T \in \F^{r \times m}$ with $S=TM$,
% such that $r$ is the rank of $M$ and $\Span(S) = \Span(M)$.
% The procedure needs at most $m(m+1)/2+r$ multiplications by a scalar
% and $m(m+1)/2$ additions of vectors in $\F^{1 \times n}$ plus
% $r(r+1)/2$ multiplications of a vector in $\F^{1 \times m}$ by a scalar
% and $r(r-1)/2$ additions of such vectors.
% \end{Prop}
% \Proof The above described procedure performs the task in the
% statement by construction. As to the costs, since
% Algorithm~\ref{AlgCleanRow} is called for all rows of $M$
% inductively and since $S$ has at most $i$ rows after step $i$,
% this needs at most $m(m+1)/2$ scalar multiplications and additions
% of vectors in $\F^{1 \times n}$. The normalisation of the
% resulting vector needs another $r$ multiplications by a scalar.
% To build up $T$ we inductively use the $a$-results of
% Algorithm~\ref{AlgCleanRow}. However, in each step, we have to multiply
% $a$ by the current matrix $T$. Since $T$ is lower triangular, the
% multiplication in step $i$ involves only $i-1$ multiplications of
% a vector in $\F^{1 \times m}$ by a scalar and $i-1$ additions of
% such vectors. Because of the multiplication by $-c$ in each step
% the total number of operations sums up to the numbers in the 
% statement. 
% \ProofEnd
% 
% \begin{Rem}
% Note that the shape of $T$ allows for further
% improvements in a practical implementation, which we ignore here.
% \end{Rem}
% 
% \vspace*{2cm}
% Alg. to compute full-echelon form
% 
% Analysis
% 
% Comparing subspaces
% 
% Nullspaces
% 
% Spinning. (fast spinning?)
% 
% 
% \subsection{Matrix inversion}
% 
% Algorithm.
% 
% Grease.
% 
% Analysis
% 
% 
\section{Cache issues}
\label{sec:cache}
 
All modern microprocessors have built in caches, most of them more than
one. For our purposes the second level cache is crucial. The potential
of proper cache usage can be read off the data in
Table~\ref{memthrough}. Varying between different processor
architectures, accesses into the cache are about $2$ to $5$ times faster
than into the main memory. Of course, these factors apply only for the
case of accessing larger areas of main memory consecutively, because
in this case modern processors use burst accesses. However, most
linear algebra operations over finite fields can be done using mostly 
such consecutive accesses. Note that these are empirical results,
theoretically, cache accesses should be a bit faster, depending on the
exact architecture.

We assume for the moment that cache accesses are $5$ times faster
than main memory accesses. If we manage to implement our linear
algebra algorithms in a way such that $90$\% of all storage accesses
are cache hits, then we can speed up $90$\% of the calculation by a
factor of $5$ (assuming that memory access is the most time-critical
part of the computation). Optimally, the remaining $10$\% remain unchanged.
This means that the overall saving factor is between $3$ and $4$ because the
needed time is $(90/5+10)\% = 28\%$ of the time without cache-aware
methods. 

However, reaping this saving factor is tricky. On one hand it proves
increasingly difficult to understand exactly, what modern processors,
memory interfaces and caches actually do. On the other hand for many
algorithms it is quite difficult to rearrange the computations in a
way, such that in fact $90\%$ of memory accesses lead to cache hits.
Quite often such rearrangements need a certain administrative effort,
which can devour parts of the performance gain.
Our observations displayed in Tables~\ref{MatMulWinograd2} and
\ref{MatMulWinograd3} provide a glance on the arising difficulties.

For the {\cvec} package we have mostly ignored the potential gains
through cache-aware programming since in most instances experiments
indicated that the necessary effort did not promise enough benefit.

On the other hand Jon Thackray has reported successes in his
stand-alone implementation of basic linear algebra routines using
cache-aware programming. His performance gains seem to lie in the
above mentioned expected range.
 
\section{Raising matrices to powers}

For the sake of completeness we conclude this chapter with the description
of a well-known technique to raise an element to a power:

\begin{Lemm}[Powering up]
If $R$ is a monoid and $x \in R$, then the power $x^m$ can be computed
in at most $2\lceil\log_2(m)\rceil$ multiplications in $R$.
\end{Lemm}
\proofbeg
Let $l := \lfloor\log_2(m)\rfloor$ and write $m$ in its binary expansion 
$m = \sum_{i=0}^l m_i 2^i$ with $m_i \in \{0,1\}$. Then
\[ x^m = \prod_{i=0, m_i \neq 0}^l x^{2^i}. \]
Computing the repeated squares $x^{2^i}$ for $1 \le i \le l$ needs $l$
multiplications in $R$ and taking the final product needs at most
another $l$ multiplications.
\proofend

\begin{Rem}
Note that there is an unpublished algorithm due to Charles Leedham-Green
in the folklore that uses the minimal polynomial and polynomial arithmetic
to raise a matrix over a finite field to an arbitrary power. Neglecting 
the time for integer arithmetic this algorithm needs for a given matrix
only a constant number of operations to compute an arbitrary power.
\end{Rem}
