% this is a part of the habilitation thesis of Max Neunhoeffer

\chapter{Introduction}

This chapter covers a few fundamental concepts and algorithms which
will be used in the rest of the book. 

\section{Straight line programs}
\label{slp}

Let $G$ be a group that is given by a tuple of generators $(g_1,
\ldots, g_k)$. That is, every element in $G$ can be expressed as a
finite product of powers of the $g_i$ and their inverses. However,
since $G$ is not necessarily abelian, the generators may occur more
than once and the products can be quite long. We call such a product a 
\emph{word in the generators $(g_1, \ldots, g_k)$}. 
For finite groups we do not need the
inverses of the generators since they all have finite order an can
thus be expressed by positive powers.

Quite often in applications we want to encode rather long words in
generators on a computer. One reason for this is that we want to store certain
elements in a known group in terms of a generating tuple (see in
particular Section~\ref{standardgens}). Another reason is for example that to
evaluate a group homomorphism $\varphi:G \to H$ on arbitrary elements
$g \in G$, if we only know the images $\varphi(g_i)$ of a tuple of
generators for $G$, we need to express $g$ explicitly in terms of
the generators $(g_1, \ldots, g_k)$. Finally, the problem of constructive
recognition (see Problem~\ref{ProbCR3}) also involves sometimes rather
long words in a tuple of generators.

To store and evaluate such words efficiently is the purpose of
\emph{straight line programs (SLP)}.

In general, straight line programs are programs that have no branches
or loops, their execution follows a ``straight line'' under all
circumstances. For the purpose of storing and evaluating words in
generators in a group, we further restrict this to the following:

\begin{Def}[Straight Line Program (SLP)]
    \label{defslp}
    A \emph{straight line program} is a procedure that takes as input a
    $k$-tuple $(g_1, \ldots, g_k)$ of group elements in a 
    common group and consists of a finite 
    sequence of steps, which are each one of the following:
\begin{itemize}
    \item Compute the product of two stored elements.
    \item Compute the inverse of a stored group element.
\end{itemize}
    An SLP starts with the elements $g_1, \ldots, g_k$,
    stores all intermediate results and returns one or more of the 
    group elements collected during its execution. The number of
    steps in the SLP is called its \emph{length}.
\end{Def}

\begin{Rem}
A straight line program of length $l$ can encode words in its input
of length up to $2^l$. Obviously, it cannot encode all words of that
length.
\end{Rem}

Implementations and data structures for straight line programs are
available in the {\GAP} (see \cite{GAP4}) and {\MAGMA} (see \cite{Magma})
computer algebra systems. The
WWW-Atlas of group representations uses straight line programs to
store generators for maximal subgroups of groups. Note that the
current implementations of straight line programs in these systems
provide an even more compact storage by allowing arbitrary finite
products of powers of previously stored group elements in each step.


\section{Randomised algorithms}
\label{montevegas}

Traditionally, an algorithm is a completely deterministic procedure to
achieve a certain goal. Whenever it is executed, it performs the same
steps and thus behaves in the same way when called twice with the same input.

However, there is a certain limitation in this paradigm. In particular
in situations, in which we want to find some result that can later
be verified to be correct easily, random methods excel. By random
methods we mean algorithms that involve a certain amount of random
choices. That is, the sequence of steps performed by a randomised
algorithm depends on certain random choices done during the algorithm.
Of course, in practice we will usually use pseudo random numbers to
do these random choices.

We do not want to go into too much detail here, but there are many
examples in which randomised algorithms can greatly outperform
deterministic algorithms. However, how do we measure or analyse
the performance of a randomised algorithm, given that it does
different things on different calls with the same inputs, and thus
has different runtimes on different occasions?

One possibility for performance analysis is to look at the average
or the expected runtime. Although this is often a good and
interesting thing to look at, this type of analysis often stays a
bit unsatisfactory, since one never knows, how long the algorithm will
take at most in a particular instance.

Therefore the most common approach for randomised algorithms is to do a 
worst-case analysis. However, clearly the absolutely worst case is
that by incredible bad luck all random choices turn out to be wrong
and the algorithm does not succeed even after a very long time. To get
rid of this problem we have to allow our algorithms to fail in some
way, most commonly simply by giving up with \textsc{Fail} as answer.
Using this exit route, we can devise algorithms that are
guaranteed to terminate after a certain number of steps or a given
amount of time. To be useful in practice, we of course want to have a
bound on the probability with which this failure occurs. Optimally, we
want to prescribe an upper bound for the failure probability
beforehand. The guaranteed upper bound for the runtime then might
depend on the choice of the failure probability bound.

In general we distinguish between so-called ``Monte-Carlo'' and
``Las Vegas'' algorithms as defined in the following.

\begin{Def}[Monte Carlo algorithm]
    A \emph{Monte Carlo algorithm with failure probability $\epsilon$}
    is a randomised algorithm that is guaranteed to terminate after
    a finite amount of time with some result, if the probability for
    returning a wrong result is bounded by $\epsilon$.
\end{Def}

A bit more satisfying is the following.

\begin{Def}[Las Vegas algorithm]
    A \emph{Las Vegas algorithm with failure probability $\epsilon$}
    is a randomised algorithm that is guaranteed to terminate after
    a finite amount of time with either the correct result or
    \textsc{Fail} indicating failure, if the probability for failure
    is bounded by $\epsilon$.
\end{Def}

The two concepts are sometimes related by the following.

\begin{Rem}[Upgrading Monte Carlo to Las Vegas by verification]
Assume that there is an efficient way to verify the correctness of the output
of a Monte Carlo algorithm. Then we can immediately upgrade the
algorithm to be of Las Vegas type by following it with a verification
step that returns ``fail'' if the result was incorrect in the first
place. ``Efficient'' here means that the verification does not take
much longer than the Monte Carlo computation in the first place.
\end{Rem}

We will use the terms ``Monte Carlo algorithm'' and ``Las Vegas
algorithm'' in this sense throughout this book.

\section{Random elements in groups}
\label{randomelts}

Randomised algorithms in group theory need random elements in groups.
Moreover, to allow a proper analysis of the behaviour of such
algorithms one needs to know quite a lot about the distribution of
the random elements in the group. Usually the best with respect to
analysis is to have a source of uniformly distributed random elements.

For most applications we are content with pseudo randomness, that is, with a
deterministic procedure which produces from some initial seed a sequence of 
elements with a good uniform distribution. Choosing other seeds (or
maybe actually choosing the seed at random) then leads to a different
behaviour of the algorithm in each call.

There are well-known methods to produce good uniformly distributed
pseudo random numbers (see for example \cite[Chapter~3]{AOCP2}).
Building on these, there is a method to produce pseudo random elements
in a finite group given by generators, which works astonishingly
well in the sense that the distribution of the elements is very close
to uniform. In the sequel we describe this method briefly but refer
for proofs to the literature. After this we discuss some of the
advantages and limitations of this method.

\begin{algorithm}
\caption{$\quad$ \sc RattleStep}
\label{rattlestep}
\begin{algorithmic}
\STATE \textbf{Input:} A pair $(a,(h_1, \ldots, h_n))$ with $a \in G$
and $G = \left< h_1, \ldots, h_n \right>$.
\STATE \textbf{Output:} A modified pair $(a,(h_1, \ldots, h_n))$ with $a \in G$
and $G = \left< \smash{h_1, \ldots, h_n} \right>$.
\vspace*{2mm}
\STATE $i := \textsc{Random}(\{ 1,2,\ldots,n\}$
\STATE $j := \textsc{Random}(\{1,2,\ldots,n-1\}$
\STATE $b := \textsc{Random}(\{1,2\}$
\IF {$j = i$}
    \STATE $j := j + 1$
\ENDIF
\IF {$b = 1$}
    \STATE $h_i := h_i \cdot h_j$
\ELSE
    \STATE $h_i := h_j \cdot h_i$
\ENDIF
\STATE $a := a \cdot h_i$
\STATE \textbf{Return} modified $(a,(h_1,\ldots,h_n))$
\end{algorithmic}
\end{algorithm}

\begin{Def}[Random elements with \textsc{Rattle}]
    \label{rattle}
Let $G$ be a group given by a tuple $(g_1, \ldots, g_k)$ of generators
and $n, N \in \N$ with $n \ge k$. The \textsc{Rattle} method to
produce random elements in $G$ is the following procedure:

It uses a variable $(a,(h_1,\ldots,h_n)) \in G \times G^n$
which is changed during the runtime by calls to
Algorithm~\ref{rattlestep}.

It first initialises $(a,(h_1,\ldots,h_n))$
by $a := \mathbf{1}_G$ and $h_i := g_i$ for $1 \le i \le k$ and $h_i
:= \mathbf{1}_G$ for $k < i \le n$.

Then it calls Algorithm~\ref{rattlestep} $N$
times with $(a,(h_1, \ldots, h_n))$ as argument
thereby changing it, and
finally returns the last value of $a$ as a random element $a_0$ in $G$.

After this initialisation phase it produces a sequence of random
elements $(a_i)_{i \in \N}$ by calling 
Algorithm~\ref{rattlestep} repeatedly with 
$(a,(h_1, \ldots, h_n))$ as argument thereby changing it and
assigning the value of $a$ to $a_i$ after step $i$.
\end{Def}

\begin{Rem}[Comments on the implementation of \textsc{Rattle}]
It is not completely clear how to choose the parameters $n$ and $N$. In
principal, $n$ could be chosen equal to $k$. However, what a good length
$N$ of the initialisation phase is depends on the group $G$ and
on the generating tuple $(g_1, \ldots, g_k)$. In practice one chooses $n$
slightly bigger than $k$ and $N$ around $100$ unless $k$ is very big.
For large $k$ the value of $N$ has to be chosen bigger. It is clear
that a constant value for $N$ will not work well for $|G| \to \infty$.
\end{Rem}

\begin{Prop}[\textsc{Rattle} converges to the uniform distribution]
    \label{proprattle}
    The distribution of the element $a_0$ in the \textsc{Rattle} procedure (see
    Definition~\ref{rattle}) converges for $N \to \infty$ to the
    uniform distribution.
\end{Prop}
\proofbeg
See \cite[Section~4]{LGMurray}.
\proofend

\subsection{A brief discussion of \textsc{Rattle}}

Although it can be proved that for every finite group $G$ and every
generating tuple $(g_1, \ldots, g_k)$ the distribution of the element
$a_0$ for $N \to \infty$ tends to the uniform distribution (see
Proposition~\ref{proprattle}), there is
not much known about the rate of convergence. So picking the value for
$N$ in practice is difficult, in particular if we do not know anything
about $G$.

If we then use the sequence produced by the \textsc{Rattle} method
after initialisation as a sequence of random elements in $G$, 
subsequent elements seem to be uniformly distributed, but adjacent
elements in the sequence are clearly not distributed independently.
Obviously the next element in the sequence depends heavily on the
previous state $(a,(h_1,\ldots,h_n))$ and the previous element is
equal to the $a$ value of this state. However, the state contains of
course more information than simply the value $a$.

Despite these obvious deficiencies, the algorithm
works surprisingly well in practice. The computational cost for the 
initialisation is 200 multiplications and after that 2 more
multiplications for every further element in the sequence.
The memory requirements are minimal and the produced sequence of
random elements is good enough for most purposes. Analysing algorithms
with the assumption that we can produce uniformly distributed random
elements in the group efficiently provides good predictions on how
well these algorithms work in practice.

Throughout this book the \textsc{Rattle} method is used to produce
random elements in group and the above assumption is made.

\section{Some notes on complexity theory}
\label{sec:complexity}

