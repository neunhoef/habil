% this is a part of the habilitation thesis of Max Neunhoeffer

\chapter{Introduction}
\label{chap:intro}

This chapter covers a few fundamental concepts and algorithms which
will be used in the rest of the book. We start talking about the
complexity of algorithms, go on with the concept of straight line
programs in groups and finish by mentioning the idea of randomisation
in algorithms and a way to produce nearly uniformly distributed
elements in a finite group.

\section{Some notes on complexity theory}
\label{sec:complexity}

\index{complexity theory}%
Already in this chapter, but all the more in the rest of the book, we
talk about the analysis of algorithms. In this section we want to
discuss briefly what we mean by this at all.

An algorithm is usually designed to solve a whole family of problems of
different sizes. Obviously, for a single, particular instance of a
computational problem one can simply store the answer and look it up
when needed in nearly no time. But this is usually not what we intend
to do when we develop an algorithm. Furthermore, it is clear that a
small instance of a computational problem usually will need less time
to solve than a big instance of the same problem. It takes for example longer 
to multiply two $10000\times 10000$-matrices with entries in the finite 
field $\F_q$ than to multiply two $100\times 100$-matrices
with entries in the same finite field $\F_q$,
even if these two situations are instances of the same
computational problem ``matrix multiplication over $\F_q$''.

Therefore, when we talk about analysing an algorithm that solves a
certain family of computational problems, we assign each instance of
this problem a ``size'' and ask how the runtime of the algorithm grows
in comparison to the size of different instances. This is in vague terms
what is meant by ``complexity of an algorithm'', and ``complexity
\index{complexity}%
theory'' is the study of the complexity of algorithms. The size of an
instance of the above mentioned matrix multiplication problem of two $n
\times n$-matrices could for example be measured by the value $n$.

Of course, different computers are running at different speeds, so
what we usually do is to count the number of steps in the algorithm
needed to complete a particular instance of the computational problem
as a function of the size of the problem instance.
We try to keep the different steps comparable. We might for example
count the number of elementary field operations (addition,
\index{elementary field operation}%
subtraction, multiplication, inversion) during the execution of a
matrix multiplication. The standard simple-minded approach to matrix
multiplication would then need $(2n-1)\cdot n^2$ elementary field
operations since each entry of the result is a sum of $n$ products of
numbers in $\F_q$, that is, we need $n$ products and $n-1$ additions
for each of the $n^2$ matrix entries. The complexity of this algorithm
in terms of the size $n$ of the input would then be $(2n-1)\cdot n^2$
elementary field operations.

If the growth rate of the complexity of an algorithm A is slower
\index{growth rate}%
than the one for another algorithm $B$ solving the same problem, then 
we would consider A to be ``better'' than B. Note that it is
possible that the complexity of B is in fact smaller than
the one of A for small problem sizes. In that case we might want to
implement both algorithms and use B for smaller instances and A for
bigger instances. In this way complexity theory helps to come up with
better implementations. If we had for example an algorithm B to multiply
two $n\times n$-matrices over $\F_q$ using $n^4/20$ elementary field
operations, it would be worthwhile to use it as opposed to the
standard algorithm A for small matrices, since
$n^4/20 \le (2n-1)\cdot n^2$ for $n \le 39$. However, for big
matrices A would outperform B dramatically.

Counting the steps in an algorithm can be very tedious and much more
difficult if there are decisions and case distinctions during the
execution. Randomisation as described in Section~\ref{montevegas}
makes this even more difficult. Therefore we are usually happy to come
up with an upper bound for the number of steps necessary. In addition,
when we are only interested in the growth rate of the complexity, we
\index{growth rate}%
are only interested in the type of function expressing this upper
bound in terms of the problem size and are actually not interested in
the constants. 

So, to determine that the growth rate of the complexity
\index{growth rate}%
of the example algorithm A above is smaller than the one of B, we do
not need the exact number of steps $(2n-1)\cdot n^2$ and $n^4/20$
respectively, but we only need to know that the first behaves ``like
$n^3$'' and the second ``like $n^4$''. The higher exponent $4$ in the
second function eventually beats the smaller constant $1/20$.
Note however that without knowing the constants we would in fact not
know that the break-even point of where we should start using A in
favour of B is at $n=39$. Having noticed this, we would like to comment
that knowing the constants in the complexity of algorithms can be very
interesting for coming up with good implementations.

Different parts of this book go differently about this. Whenever
possible we have tried to include constants in the complexity
estimates. However, sometimes this would have been too tedious and
would have complicated things dramatically. In these cases we are
content with determining the growth rate. For this purpose, we adopt
\index{growth rate}%
the standard big-$O$ notations, which we will briefly repeat here.
It is used to formulate statements about the asymptotic behaviour of
functions.

\begin{Def}[Capital-$O$-notation]
    \label{capO}\index{O-notation@$O(-)$-notation}%
    Let $\R^+$ be the set of positive real numbers and
    $g : \R^+ \to \R$.
    We say that a function $f : \R^+ \to \R$ is $O(g)$ if there are
    two positive real constants $C$ and $D$ such that 
    $|f(x)| \le C \cdot |g(x)|$ for all $x > D$.
\end{Def}

This will be used in the analysis of algorithms by saying for
example: Algorithm A above has complexity $O(n^3)$ whereas algorithm B
\index{complexity}%
above has complexity $O(n^4)$. Obviously it has to be clear from the
context, which family of computational problems the algorithm deals
with and how the size $n$ of an instance of the problem is measured.

There is a certain sloppiness in this usage of Definition~\ref{capO}.
Strictly speaking the following statement is true as well: 
``The function $n \mapsto (2n-1)\cdot n^2$ is $O(n^5)$.'' This follows
from the fact that the function $n \mapsto n^3$ is $O(n^5)$. However,
this statement is not as strong as possible. Obviously, we are
interested in the fact that the exponent $3$ is the \emph{least
possible $e$} for which the statement ``the complexity of algorithm A is
$O(n^e)$'' is true. Sometimes we can prove that our statement is best
possible and sometimes we cannot.

Up to now we have concentrated on the ``time complexity'' of
\index{time complexity}%
algorithms, that is, the asymptotic behaviour of the runtime with
growing problem sizes. Although this is usually the most interesting
aspect, it is of course also necessary to keep an eye on other
computational resources like memory usage. We speak of ``space
complexity'' in this case and use a similar setup and language. This
\index{space complexity}%
only plays a minor role in this book.

The point of view we adopt in this book is that the complexity analysis of
algorithms is a tool to come up with good implementations. Complexity
results tell us, which algorithms are suited best for which problem
classes and possibly which problem sizes, and they tell us, what is
considered to be a ``satisfactory'' algorithm for a problem class and
what is considered to be ``lacking''. In general we would like to
devise algorithms which have a polynomial as an asymptotic bound of their
time complexity in terms of the problem size. We call these algorithms
``polynomial-time algorithms''.


\section{Straight line programs}
\label{slp}

\index{straight line program}\index{SLP}%
Let $G$ be a group that is given by a tuple of generators $(g_1,
\ldots, g_k)$. That is, every element in $G$ can be expressed as a
finite product of powers of the $g_i$ and their inverses. However,
since $G$ is not necessarily abelian, the generators may occur more
than once and the products can be quite long. We call such a product a 
\emph{word in the generators $(g_1, \ldots, g_k)$}. 
\index{word in generators}%
For finite groups we do not need the
inverses of the generators since they all have finite order and can
thus be expressed by positive powers.

Quite often in applications we want to encode rather long words in
generators on a computer. One reason for this is that we want to store certain
elements in a known group in terms of a generating tuple (see in
particular Section~\ref{standardgens}). Another reason is for example that to
evaluate a group homomorphism $\varphi:G \to H$ on arbitrary elements
$g \in G$, if we only know the images $\varphi(g_i)$ of a tuple of
generators for $G$, we need to express $g$ explicitly in terms of
the generators $(g_1, \ldots, g_k)$. Finally, the problem of constructive
recognition (see Problem~\ref{ProbCR3}) involves sometimes rather
\index{constructive recognition}%
long words in a tuple of generators, too.

To store and evaluate such words efficiently is the purpose of
\emph{straight line programs (SLP)}.

In general, straight line programs are programs that have no branches
or loops, their execution follows a ``straight line'' under all
circumstances. For the purpose of storing and evaluating words in
generators in a group, we further restrict this to the following:

\begin{Def}[Straight Line Program (SLP)]
    \label{defslp}\index{straight line program}\index{SLP}%
    \index{straight line program!definition of}\index{SLP!definition of}%
    A \emph{straight line program} is a procedure that takes as input a
    $k$-tuple $(g_1, \ldots, g_k)$ of group elements in a 
    common group and consists of a finite 
    sequence of steps, which are each one of the following:
\begin{itemize}
    \item Compute the product of two stored elements, or
    \item compute the inverse of a stored group element.
\end{itemize}
    An SLP starts with the elements $g_1, \ldots, g_k$,
    stores all intermediate results and returns one or more of the 
    group elements collected during its execution. The number of
    steps in the SLP is called its \emph{length}.
\end{Def}

\begin{Rem}
A straight line program of length $l$ can encode words in its input
of length up to $2^l$. Obviously, it cannot encode all words of that
length.
\end{Rem}

Implementations and data structures for straight line programs are
available in the {\GAP} (see \cite{GAP4}) and {\MAGMA} (see
\cite{Magma})
computer algebra systems. The
WWW-Atlas of group representations uses straight line programs to
\index{WWW-Atlas of group representations}%
store generators for maximal subgroups of groups. Note that the
current implementations of straight line programs in these systems
provide an even more compact storage by allowing arbitrary finite
products of powers of previously stored group elements in each step.


\section{Randomised algorithms}
\label{montevegas}
\index{randomised algorithm}\index{algorithm!randomised}%

Traditionally, an algorithm is a completely deterministic procedure to
achieve a certain goal. Whenever it is executed, it performs the same
steps and thus behaves in the same way when called twice with the same input.

However, there is a certain limitation in this paradigm. In particular
in situations, in which we want to find some result that can later
be verified to be correct easily, randomised methods excel. By
randomised
methods we mean algorithms that involve a certain amount of random
choices. That is, the sequence of steps performed by a randomised
algorithm depends on certain random choices done during the algorithm.
Of course, in practice we will usually use pseudo random numbers to
do these random choices.

We do not want to go into too much detail here, but there are many
examples in which randomised algorithms can greatly outperform
deterministic algorithms. However, how do we measure or analyse
the performance of a randomised algorithm, given that it does
different things on different calls with the same input, and thus
has different runtimes on different occasions?

One possibility for performance analysis is to look at the average
or the expected runtime. Although this is a good and
interesting thing to look at, this type of analysis often stays a
bit unsatisfactory, since one never knows, how long the algorithm will
take at most in a particular instance.

Therefore the most common approach for randomised algorithms is to do a 
worst-case analysis. However, clearly the absolutely worst case is
that by incredible bad luck all random choices turn out to be wrong
and the algorithm does not succeed even after a very long time. To get
rid of this problem we have to allow our algorithms to fail in some
way, most commonly simply by giving up with \textsc{Fail} as answer
after a certain time.
Using this exit route, we can devise algorithms that are
guaranteed to terminate after a certain number of steps or a given
amount of time. To be useful in practice, we of course want to have a
bound for the probability with which this failure occurs. Optimally, we
want to prescribe an upper bound for the failure probability
beforehand. The guaranteed upper bound for the runtime then might
depend on the choice of the failure probability bound.

In general we distinguish between so-called ``Monte Carlo'' and
``Las Vegas'' algorithms as defined in the following.

\begin{Def}[Monte Carlo algorithm]
\index{randomised algorithm}\index{algorithm!randomised}%
\index{Monte Carlo algorithm}%
    A \emph{Monte Carlo algorithm with failure probability $\epsilon$}
    is a randomised algorithm that is guaranteed to terminate after
    a finite amount of time with some result, if the probability for
    returning a wrong result is bounded by $\epsilon$.
\end{Def}

A bit more satisfying is the following.

\begin{Def}[Las Vegas algorithm]
\index{randomised algorithm}\index{algorithm!randomised}%
\index{Las Vegas algorithm}%
    A \emph{Las Vegas algorithm with failure probability $\epsilon$}
    is a randomised algorithm that is guaranteed to terminate after
    a finite amount of time with either the correct result or
    \textsc{Fail} indicating failure, if the probability for failure
    is bounded by $\epsilon$.
\end{Def}

The two concepts are sometimes related by the following.

\begin{Rem}[Upgrading Monte Carlo to Las Vegas by verification]
\index{verification}%

Assume that there is an efficient way to verify the correctness of the output
of a Monte Carlo algorithm. Then we can immediately upgrade the
algorithm to be of Las Vegas type by following it with a verification
step that returns \textsc{Fail}, if the result was incorrect in the first
place. ``Efficient'' here means that the verification does not take
much longer than the Monte Carlo computation in the first place.
\end{Rem}

We will use the terms ``Monte Carlo algorithm'' and ``Las Vegas
algorithm'' in this sense throughout this book.

\section{Random elements in finite groups}
\label{randomelts}

Randomised algorithms in group theory need random elements in groups.
Moreover, to allow a proper analysis of the behaviour of such
algorithms one needs to know quite a lot about the distribution of
the random elements in the group. Usually, the best with respect to
analysis is to have a source of uniformly distributed random elements.

For most applications we are content with pseudo randomness, that is, with a
deterministic procedure which produces from some initial seed a sequence of 
elements with a good uniform distribution. Choosing different seeds (or
maybe actually choosing the seed at random) then leads to a different
behaviour of the algorithm in each call.

There are well-known methods to produce good uniformly distributed
pseudo random numbers (see for example \cite[Chapter~3]{AOCP2}).
Building on these, there is a method to produce pseudo random elements
in a finite group given by generators, which works astonishingly
well in the sense that the distribution of the elements is very close
to uniform. In the sequel we describe this method briefly but refer
for proofs to the literature. After this we discuss some of the
advantages and limitations of this method.

\begin{algorithm}
\caption{$\quad$ \sc RattleStep}
\label{rattlestep}
\index{Rattle@\textsc{Rattle}}%
\begin{algorithmic}
\STATE \textbf{Input:} A pair $(a,(h_1, \ldots, h_n))$ with $a \in G$
and $G = \left< h_1, \ldots, h_n \right>$.
\STATE \textbf{Output:} A modified pair $(a,(h_1, \ldots, h_n))$ with $a \in G$
and $G = \left< \smash{h_1, \ldots, h_n} \right>$.
\vspace*{2mm}
\STATE $i := \textsc{Random}(\{ 1,2,\ldots,n\}$
\STATE $j := \textsc{Random}(\{1,2,\ldots,n-1\}$
\STATE $b := \textsc{Random}(\{1,2\}$
\IF {$j = i$}
    \STATE $j := j + 1$
\ENDIF
\IF {$b = 1$}
    \STATE $h_i := h_i \cdot h_j$
\ELSE
    \STATE $h_i := h_j \cdot h_i$
\ENDIF
\STATE $a := a \cdot h_i$
\STATE \textbf{Return} modified $(a,(h_1,\ldots,h_n))$
\end{algorithmic}
\end{algorithm}

\begin{Def}[Random elements with \textsc{Rattle}]
    \label{rattle}
\index{Rattle@\textsc{Rattle}}%
Let $G$ be a group given by a tuple $(g_1, \ldots, g_k)$ of generators
and $n, N \in \N$ with $n \ge k$. The \textsc{Rattle} method to
produce random elements in $G$ is the following procedure:

It uses a variable $(a,(h_1,\ldots,h_n)) \in G \times G^n$
which is changed during the runtime by calls to
Algorithm~\ref{rattlestep}.

It first initialises $(a,(h_1,\ldots,h_n))$
by $a := \mathbf{1}_G$ and $h_i := g_i$ for $1 \le i \le k$ and $h_i
:= \mathbf{1}_G$ for $k < i \le n$.

Then it calls Algorithm~\ref{rattlestep} $N$
times with $(a,(h_1, \ldots, h_n))$ as argument
thereby changing it and
finally returns the last value of $a$ as a random element $a_0$ in $G$.

After this initialisation phase it produces a sequence of random
elements $(a_i)_{i \in \N}$ by calling 
Algorithm~\ref{rattlestep} repeatedly with 
$(a,(h_1, \ldots, h_n))$ as argument thereby changing it and
assigning the value of $a$ to $a_i$ after call number $i$.
\end{Def}

\begin{Rem}[Variant of product replacement]
\index{Rattle@\textsc{Rattle}}%
The \textsc{Rattle} method described above is a variant of the
``product replacement algorithm'', because the main part of a step replaces 
an element by a product of it with another one.
\end{Rem}

\begin{Rem}[Comments on the implementation of \textsc{Rattle}]
\index{Rattle@\textsc{Rattle}}%
It is not completely clear how to choose the parameters $n$ and $N$. In
principal $n$ could be chosen equal to $k$. However, what a good length
$N$ of the initialisation phase is depends on the group $G$ and
on the generating tuple $(g_1, \ldots, g_k)$. In practice one chooses $n$
slightly bigger than $k$ and $N$ around $100$ unless $k$ is very big.
For large $k$ the value of $N$ has to be chosen bigger. It is clear
that a constant value for $N$ will not work well for $|G| \to \infty$.
\end{Rem}

\begin{Prop}[\textsc{Rattle} converges to the uniform distribution]
    \label{proprattle}
\index{Rattle@\textsc{Rattle}}%
    The distribution of the element $a_0$ in the \textsc{Rattle} procedure (see
    Definition~\ref{rattle}) converges for $N \to \infty$ to the
    uniform distribution.
\end{Prop}
\proofbeg
See \cite[Section~4]{LGMurray}.
\proofend

\subsubsection{A brief discussion of \textsc{Rattle}}

\index{Rattle@\textsc{Rattle}}%
Although it can be proved that for every finite group $G$ and every
generating tuple $(g_1, \ldots, g_k)$ the distribution of the element
$a_0$ for $N \to \infty$ tends to the uniform distribution (see
Proposition~\ref{proprattle}), there is
not much known about the rate of convergence. So, picking the value for
$N$ in practice is difficult, in particular if we do not know anything
about $G$.

If we use the sequence produced by the \textsc{Rattle} method
after initialisation as a sequence of random elements in $G$, 
subsequent elements seem to be uniformly distributed, but adjacent
elements in the sequence are clearly not distributed independently.
Obviously the next element in the sequence depends heavily on the
previous state $(a,(h_1,\ldots,h_n))$ and the previous element is
equal to the $a$ value of this state. However, the state contains of
course more information than simply the value $a$.

Despite these obvious deficiencies, the algorithm
works surprisingly well in practice. The computational cost for the 
initialisation is 200 multiplications and after that 2 more
multiplications for every further element in the sequence.
The memory requirements are minimal and the produced sequence of
random elements is good enough for most purposes. Analysing algorithms
that use \textsc{Rattle}
\index{Rattle@\textsc{Rattle}}%
with the assumption that it produces uniformly distributed random
elements in the group provides good predictions on how
well these algorithms work in practice.

Throughout this book the \textsc{Rattle} method is used to produce
random elements in group and the above assumption is made.
\index{Rattle@\textsc{Rattle}}%

